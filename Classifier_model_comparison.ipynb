{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier_model_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1usCd_mkB9E1jNfsRT7KRU-QO3Nio-3rx",
      "authorship_tag": "ABX9TyNlmWB0hWBVYR9dzgTp9jOT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnEIO4Srqa9r",
        "outputId": "d9979b3d-d188-45a2-e742-cdd0e7cf7c25"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct  2 15:50:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqkpiR-5qnYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ef71c5-e0da-4d14-bcde-d3fd09e48821"
      },
      "source": [
        "!unzip /content/drive/MyDrive/RealTime_Face_Detector/augmentedDataset.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/RealTime_Face_Detector/augmentedDataset.zip\n",
            "   creating: /content/content/dataset/\n",
            "   creating: /content/content/dataset/trainImages/\n",
            "   creating: /content/content/dataset/trainImages/not_me/\n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0006.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Sorkin_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0012.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Peirsol_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdoulaye_Wade_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0019.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_0003.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Guiel_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Patterson_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0010.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Alan_Ball_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Tippin_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdel_Nasser_Assidi_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Peirsol_0003.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdul_Rahman_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdoulaye_Wade_0004.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0013.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Sorkin_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0016.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdoulaye_Wade_0003.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abbas_Kiarostami_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0015.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Pena_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdulaziz_Kamilov_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0011.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Peirsol_0004.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdel_Madi_Shabneh_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdul_Majeed_Shobokshi_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdel_Aziz_Al-Hakim_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0005.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Ahmad_Badawi_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0018.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_al-Attiyah_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0007.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Peirsol_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0009.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Aaron_Eckhart_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdoulaye_Wade_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0003.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0004.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0017.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdel_Nasser_Assidi_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0008.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_al-Attiyah_0002.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_Gul_0014.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_0004.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abdullah_al-Attiyah_0003.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/not_me/Abba_Eban_0001.jpg  \n",
            "   creating: /content/content/dataset/trainImages/me/\n",
            "  inflating: /content/content/dataset/trainImages/me/IMG-20190822-WA0001.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1294.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200308_125514.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG-20181219-WA0021.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20201005_132205964.NIGHT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20201207_044839571.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1291.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210227_033714412.PORTRAIT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/instasize_210522184941.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1295.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/RenderedImage.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200318_003322.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200320_162135.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210227_034121661.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20190321_194149.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1285.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_0760.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20181214_221109.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1286.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210227_033831727.PORTRAIT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1192.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG-20210226-WA0034.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20201025_112313_670.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200503_170857_571.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200203_221455~2.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_0198.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1287.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20210522_185316_651.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1275.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200917_204645_762.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG-20181111-WA0007.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG-20190908-WA0000.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200503_203755.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1293.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20201005_132428279.NIGHT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_0437.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210325_035310926.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1015.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_0629.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1273.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20201005_132422131.NIGHT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210203_044559415.PORTRAIT~3.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/instasize_210522184755.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20201005_132457646.NIGHT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1006.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200320_162202.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1274.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1193.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20201005_132443471.NIGHT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20181214_221239.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG-20190822-WA0003.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20210209_205906_209.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20190321_202612.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1014.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200321_094118.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20191208_131240.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200307_180226.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_0229.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1292.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210124_065626543.PORTRAIT.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_0265.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210227_033638056.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/PXL_20210124_065547962.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_2799.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_0197.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/Snapchat-1180251034.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_1290.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/IMG_20200904_212049~3.jpg  \n",
            "  inflating: /content/content/dataset/trainImages/me/Snapchat-1834820744~2.jpg  \n",
            "   creating: /content/content/dataset/face_dataset_train_aug_image/\n",
            "   creating: /content/content/dataset/face_dataset_train_aug_image/me/\n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_3390.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_9423.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_6856.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_5215.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_7785.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_3571.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_3836.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_4150.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_3990.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_1184.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_5738.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_7326.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_1591.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_1688.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_324.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_2173.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_5083.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_2012.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_8486.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_1526.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_9383.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_2626.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_3486.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_7105.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_4632.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_8757.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_6037.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_3481.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_2198.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_5903.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_2416.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_7164.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_4928.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_9605.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_2174.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_6508.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_5279.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_2389.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_2419.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_6486.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_2571.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_5106.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_5945.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_994.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_4658.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_6662.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_6203.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_8213.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_215.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_9649.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_6478.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_3.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_3927.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_3949.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_2883.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_1108.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_8875.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_9985.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_2140.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_8442.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_3030.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_337.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_4450.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_6713.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_8062.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_8725.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_9651.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_1823.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_2791.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_186.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_7133.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_2126.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_2838.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_5275.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_9779.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_181.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_6337.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_6598.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_1904.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_3606.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_7123.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_7720.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_9249.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_9832.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_5225.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_1195.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_985.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_3431.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_4619.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_2841.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_8872.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_1626.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_2315.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_138.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_6330.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_5250.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_4694.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_3406.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_374.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_8563.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_8962.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_4911.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_2454.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_281.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_711.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_5689.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_9174.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_7280.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_7013.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_2487.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_3869.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_7613.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_5036.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_799.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_7227.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_9944.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_400.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_7736.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_7439.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_2709.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_7054.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_1466.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_3168.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_3815.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_9722.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_4635.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_6180.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_3161.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_1726.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_8152.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_3727.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_7181.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_7379.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_8676.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_1982.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_4299.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_1038.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_4884.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_7502.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_5291.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_6221.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_1722.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_8471.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_5122.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_8925.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_7964.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_2736.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_6644.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_2121.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_5235.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_463.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_3228.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_5524.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_7365.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_5495.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_9864.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_9202.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_5061.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_9189.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_4579.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_1926.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_8890.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_4556.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_4167.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_4136.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_4010.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_3265.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_9445.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_4621.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_347.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_9046.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_3616.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_8351.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_8224.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_5635.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_5884.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_8162.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_5470.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_3402.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_7425.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_7108.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_7402.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_8349.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_1186.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_7409.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_544.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_5697.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_4977.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_2832.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_7676.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_3107.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_6418.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_7836.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_7937.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_8223.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_6910.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_1057.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_7709.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_2865.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_1223.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_7293.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_9489.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_2375.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_7631.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_8349.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_1350.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_2264.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_6.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_4937.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_6449.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_8628.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_7815.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_4268.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_7285.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_5187.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_8326.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_4872.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_8939.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_3188.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_3219.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_5279.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_9279.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_1937.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_1388.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_5253.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_4156.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_166.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_3323.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_9816.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_899.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_6225.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_1159.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_5630.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_6556.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_3424.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_4881.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_7969.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_1255.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_5167.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_4292.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_6004.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_5913.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_9110.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_9954.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_2629.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_3237.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_2795.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_9560.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_869.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_6086.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_5246.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_2492.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_4513.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_178.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_2332.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_1442.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_3788.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_303.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_6257.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_3224.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_4.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_4636.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_2673.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_2652.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_2949.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_6077.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_9614.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_8136.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_398.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_1742.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_8255.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_4240.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_2961.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_7400.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_279.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_8765.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_9480.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_9401.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_705.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_3002.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_8874.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_9698.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_3061.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_2492.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_9143.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_3689.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_272.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_2044.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_6164.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_391.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_4347.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_3588.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_6085.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_9189.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_1176.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_6978.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_8492.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_5198.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_2224.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_9310.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_6132.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_3147.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_132.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_895.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_6292.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_6297.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_6828.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_5968.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_284.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_5670.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_4702.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_1383.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_5886.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_4856.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_5737.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_5082.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_3372.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_7037.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_4035.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_6420.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_5846.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_6375.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_7176.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_9050.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_8698.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_5863.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_8383.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_738.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_577.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_6205.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_7166.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_184.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_5571.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_3641.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_5460.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_1200.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_9427.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_5919.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_8122.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_8693.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_1116.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_7193.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_1243.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_129.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_7685.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_5515.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_1562.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_3871.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_1604.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_5680.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_1683.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_1251.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_3248.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_1966.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_7507.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_4562.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_3958.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_6837.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_8886.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_626.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_6773.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_3717.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_7480.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_4152.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_6853.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_5352.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_7881.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_304.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_4331.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_9866.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_3924.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_7443.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_3510.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_8468.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_8926.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_3822.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_9126.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_1891.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_7269.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_8921.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_5350.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_344.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_3994.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_5508.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_8677.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_2545.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_645.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_895.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_4614.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_8454.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_7006.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_9474.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_9491.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_3550.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_7040.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_8097.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_9665.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_1301.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_5039.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_8472.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_3924.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_396.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_7241.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_6259.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_4724.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_7861.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_6896.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_335.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_9906.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_2173.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_3817.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_9073.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_6477.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_9424.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_2161.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_597.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_4595.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_3674.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_6000.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_2970.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_7346.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_7726.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_19.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_1346.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_658.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_9991.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_4469.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_5534.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_9688.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_1319.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_2796.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_3450.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_2641.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_2025.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_9773.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_6774.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_7950.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_5551.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_9474.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_936.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_1738.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_5822.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_3775.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_2338.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_7447.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_7651.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_836.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_2338.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_8882.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_4091.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_8751.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_5000.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_8759.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_325.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_5306.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_8160.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_1282.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_4391.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_9777.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_8697.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_4091.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_3361.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_9923.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_4373.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_7257.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210203_044559415.PORTRAIT~3.jpg_0_2272.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_7571.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_3451.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_3087.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_8915.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_3538.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_9568.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_678.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_2145.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_8924.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_3717.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_5934.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_336.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_5092.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_5491.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_1150.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_6308.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_2461.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_1884.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_9060.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210325_035310926.jpg_0_543.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_7236.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_4142.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_5603.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_3235.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_2454.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_7147.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_118.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_3049.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_6314.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_4031.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_8469.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_7687.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_1036.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1292.jpg_0_7043.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_688.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_2740.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_979.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_5642.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_4461.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_336.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_2759.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_6814.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_7181.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_6570.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_6122.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_809.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_152.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_5253.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_5256.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_9035.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_1937.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_4111.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_2574.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_32.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_239.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_1883.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_9183.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_2470.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_819.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_1865.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_7816.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_2220.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_2615.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0229.jpg_0_775.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_9401.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_849.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_6568.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_6222.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_2876.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_263.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_1444.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_9184.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_6609.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_6771.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_5603.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_8221.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_9987.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_7459.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_4704.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_6530.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_8339.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_1505.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_3304.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_5587.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_5820.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_3867.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_497.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_3423.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_8258.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_7919.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1193.jpg_0_503.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_9697.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_3075.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_7256.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_6719.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_5157.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_1728.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_4118.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_2160.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_8651.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_8391.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_5957.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_3374.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_4990.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_412.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_1752.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_3430.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_7916.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_2656.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_9528.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033831727.PORTRAIT.jpg_0_9932.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_5045.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_6195.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_1970.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_3345.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_8058.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_7166.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_8089.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_7460.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_2585.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0197.jpg_0_5662.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_034121661.jpg_0_886.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184755.jpg_0_7628.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_3935.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_9690.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1014.jpg_0_184.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_8610.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1290.jpg_0_6754.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1273.jpg_0_323.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_8575.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1015.jpg_0_3151.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_6891.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_7983.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1293.jpg_0_8020.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1287.jpg_0_4975.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_1630.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_8557.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_5396.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_7953.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_1251.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132428279.NIGHT.jpg_0_9484.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_1444.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065547962.jpg_0_8661.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_194149.jpg_0_6959.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_7829.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_2367.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_1476.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_1298.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_9115.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_2429.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_2880.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_5859.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_2799.jpg_0_8906.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210124_065626543.PORTRAIT.jpg_0_4891.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1275.jpg_0_5196.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20190321_202612.jpg_0_8157.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200203_221455~2.jpg_0_260.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_1534.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0198.jpg_0_2918.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_7869.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201115_070223248.PORTRAIT-01.COVER.jpg_0_350.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_5782.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_8307.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_716.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0265.jpg_0_4357.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/instasize_210522184941.jpg_0_4010.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_853.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_2554.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_8035.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1274.jpg_0_1179.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_9977.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_6673.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_4436.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_7736.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200917_204645_762.jpg_0_977.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_8834.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_8949.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200307_180226.jpg_0_7189.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_3131.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181111-WA0007.jpg_0_3697.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_3408.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_5342.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1006.jpg_0_8104.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/RenderedImage.jpg_0_1077.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190908-WA0000.jpg_0_6831.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033638056.jpg_0_2857.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_8300.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_9636.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_8001.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20181219-WA0021.jpg_0_9875.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1285.jpg_0_7663.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132457646.NIGHT.jpg_0_2841.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1291.jpg_0_823.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1192.jpg_0_4518.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_6242.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_170857_571.jpg_0_7557.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_5824.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200321_094118.jpg_0_5388.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_1762.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0003.jpg_0_3686.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_860.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0629.jpg_0_2951.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20210226-WA0034.jpg_0_7292.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132443471.NIGHT.jpg_0_1337.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132422131.NIGHT.jpg_0_2840.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20191208_131240.jpg_0_5148.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_5533.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_4747.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_1757.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG-20190822-WA0001.jpg_0_7798.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162135.jpg_0_72.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221109.jpg_0_9622.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1834820744~2.jpg_0_4005.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_8316.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/Snapchat-1180251034.jpg_0_3258.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1294.jpg_0_1494.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20181214_221239.jpg_0_4919.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201207_044839571.jpg_0_5898.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_8280.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20201025_112313_670.jpg_0_6417.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210522_185316_651.jpg_0_5732.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20210209_205906_209.jpg_0_6015.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20210227_033714412.PORTRAIT.jpg_0_3.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200904_212049~3.jpg_0_442.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/PXL_20201005_132205964.NIGHT.jpg_0_4564.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_8560.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1286.jpg_0_5775.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200308_125514.jpg_0_3312.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200503_203755.jpg_0_6551.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_1295.jpg_0_6285.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0760.jpg_0_7664.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_0437.jpg_0_581.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200320_162202.jpg_0_385.jpg  \n",
            "  inflating: /content/content/dataset/face_dataset_train_aug_image/me/IMG_20200318_003322.jpg_0_4342.jpg  \n",
            "   creating: /content/content/dataset/testImages/\n",
            "   creating: /content/content/dataset/testImages/not_me/\n",
            "  inflating: /content/content/dataset/testImages/not_me/Aitor_Gonzalez_0002.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Akbar_Hashemi_Rafsanjani_0003.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Akbar_Hashemi_Rafsanjani_0001.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Aitor_Gonzalez_0001.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Alan_Ball_0001.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Aicha_El_Ouafi_0001.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Aicha_El_Ouafi_0002.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Akbar_Hashemi_Rafsanjani_0002.jpg  \n",
            "  inflating: /content/content/dataset/testImages/not_me/Aicha_El_Ouafi_0003.jpg  \n",
            "   creating: /content/content/dataset/testImages/me/\n",
            "  inflating: /content/content/dataset/testImages/me/FB_IMG_1543004173806.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/18f3ca8e-f3d6-4020-812b-1800b5b67c15~2.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/IMG_0007.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/FB_IMG_1543004107327.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/IMG_0004.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/FB_IMG_1543004113291.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/IMG_0077.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/a8e5e4f3-4b4b-4d22-ade3-1acd6c74bc10.jpg  \n",
            "  inflating: /content/content/dataset/testImages/me/FB_IMG_1543004123855.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehb_nvfGq71R"
      },
      "source": [
        "# Common Imports\n",
        "import os \n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZE2TWFNvg80"
      },
      "source": [
        "# Tensorflow imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_teQs6Ecv1Is"
      },
      "source": [
        "# Dataset Information\n",
        "train_aug_image_folder = os.path.join('dataset','face_dataset_train_aug_image')\n",
        "train_image_folder = os.path.join('dataset','trainImages')\n",
        "test_image_folder = os.path.join('dataset','testImages')\n",
        "img_height, img_width = 250,250\n",
        "num_classes = 2\n",
        "\n",
        "validation_ratio=0.2\n",
        "batch_size=20\n",
        "AUTOTUNE=tf.data.AUTOTUNE"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Pd6vdywzKW"
      },
      "source": [
        "Creation of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLOlZnA8wxiQ",
        "outputId": "1a1e3ffd-04fe-4f63-98ed-1f26d70aebfe"
      },
      "source": [
        "# Train and validation sets of initial dataset\n",
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    train_image_folder,\n",
        "    validation_split=validation_ratio,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=(img_height,img_width),\n",
        "    label_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True    \n",
        ")\n",
        "\n",
        "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    train_image_folder,\n",
        "    validation_split=validation_ratio,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=(img_height,img_width),\n",
        "    label_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 122 files belonging to 2 classes.\n",
            "Using 98 files for training.\n",
            "Found 122 files belonging to 2 classes.\n",
            "Using 24 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkQ5taGAyD-q",
        "outputId": "f9edca8b-92d7-4a29-a876-52c14f26ec35"
      },
      "source": [
        "# Train and validation sets of augmented dataset\n",
        "train_aug_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    train_aug_image_folder,\n",
        "    seed=42,\n",
        "    image_size=(img_height,img_width),\n",
        "    label_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# val_aug_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "#     train_aug_image_folder,\n",
        "#     validation_split=validation_ratio,\n",
        "#     subset=\"validation\",\n",
        "#     seed=42,\n",
        "#     image_size=(img_height, img_width),\n",
        "#     batch_size=batch_size,\n",
        "#     label_mode='categorical',\n",
        "#     shuffle=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 700 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU70BpuczB47",
        "outputId": "334aa6bc-9c69-420c-f34f-de013cfb0ec5"
      },
      "source": [
        "test_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    test_image_folder,\n",
        "    image_size=(img_height,img_width),\n",
        "    label_mode='categorical',\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 18 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o0Zf4W2zT-Y",
        "outputId": "9b66327d-7f5e-4185-ab6d-667d5459ed79"
      },
      "source": [
        "class_names = test_ds.class_names\n",
        "class_names"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['me', 'not_me']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ova-jMV4zeHB"
      },
      "source": [
        "Building Model with Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH73qwEFznRV"
      },
      "source": [
        "VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNWrXWR2zkR8",
        "outputId": "bd62c5e8-4093-4404-d4d2-2ae6dd6ef9df"
      },
      "source": [
        "base_model = keras.applications.vgg16.VGG16(weights='imagenet',include_top=False,input_shape=(img_height,img_width,3))\n",
        "# Set layers to non-trainable\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "# Add custom layers on top of the convolutional layers of VGG16\n",
        "flatten = keras.layers.Flatten()(base_model.output)\n",
        "dense_4096_1 = keras.layers.Dense(4096,activation='relu')(flatten)\n",
        "dense_4096_2 = keras.layers.Dense(4096,activation='relu')(dense_4096_1)\n",
        "output = keras.layers.Dense(num_classes,activation='sigmoid')(dense_4096_2)\n",
        "\n",
        "VGG16 = keras.models.Model(inputs=base_model.input, outputs=output,name='VGG16')\n",
        "VGG16.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"VGG16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 250, 250, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 250, 250, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 250, 250, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 125, 125, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 125, 125, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 125, 125, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 62, 62, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 62, 62, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 62, 62, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 31, 31, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 134,268,738\n",
            "Trainable params: 119,554,050\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvGCW8pE1HAs"
      },
      "source": [
        "ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biPub5ShzbbT",
        "outputId": "77653b0d-b448-4ecf-9d4c-334378922d3b"
      },
      "source": [
        "base_model = keras.applications.ResNet50(weights='imagenet',include_top=False,input_shape=(img_height,img_width,3))\n",
        "\n",
        "# Set layer to non-trainable\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "# Add custom layers on top of ResNet50\n",
        "global_avg_pooling = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(num_classes,activation='sigmoid')(global_avg_pooling)\n",
        "\n",
        "ResNet50 = keras.models.Model(inputs=base_model.input,outputs=output,name='ResNet50')\n",
        "ResNet50.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ResNet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 250, 250, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 256, 256, 3)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 125, 125, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 125, 125, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 125, 125, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 127, 127, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 63, 63, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 63, 63, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 63, 63, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 63, 63, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 63, 63, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 63, 63, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 63, 63, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 63, 63, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 63, 63, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 63, 63, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 63, 63, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 63, 63, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 63, 63, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 63, 63, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 63, 63, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 2)            4098        global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6Nd5Q_T2RdN"
      },
      "source": [
        "ResNet152"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50sbo5If18YL",
        "outputId": "b8ec42c8-061a-45ca-99de-7b0bc46ba189"
      },
      "source": [
        "base_model=keras.applications.ResNet152(weights='imagenet',include_top=False,input_shape=(img_height,img_width,3))\n",
        "\n",
        "# Set layers to non-trainable\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "# Add custom layers on top of ResNet\n",
        "global_avg_pooling = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(num_classes,activation='sigmoid')(global_avg_pooling)\n",
        "\n",
        "ResNet152 = keras.models.Model(inputs=base_model.input,outputs=output,name='ResNet152')\n",
        "ResNet152.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ResNet152\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 250, 250, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 256, 256, 3)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 125, 125, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 125, 125, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 125, 125, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 127, 127, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 63, 63, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 63, 63, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 63, 63, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 63, 63, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 63, 63, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 63, 63, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 63, 63, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 63, 63, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 63, 63, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 63, 63, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 63, 63, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 63, 63, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 63, 63, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 63, 63, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 63, 63, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 32, 32, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_relu (Activation (None, 32, 32, 128)  0           conv3_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_add (Add)          (None, 32, 32, 512)  0           conv3_block4_out[0][0]           \n",
            "                                                                 conv3_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_out (Activation)   (None, 32, 32, 512)  0           conv3_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 32, 32, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_relu (Activation (None, 32, 32, 128)  0           conv3_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_add (Add)          (None, 32, 32, 512)  0           conv3_block5_out[0][0]           \n",
            "                                                                 conv3_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_out (Activation)   (None, 32, 32, 512)  0           conv3_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 32, 32, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_relu (Activation (None, 32, 32, 128)  0           conv3_block7_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block7_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block7_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_add (Add)          (None, 32, 32, 512)  0           conv3_block6_out[0][0]           \n",
            "                                                                 conv3_block7_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_out (Activation)   (None, 32, 32, 512)  0           conv3_block7_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block7_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 32, 32, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_relu (Activation (None, 32, 32, 128)  0           conv3_block8_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block8_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block8_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_add (Add)          (None, 32, 32, 512)  0           conv3_block7_out[0][0]           \n",
            "                                                                 conv3_block8_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_out (Activation)   (None, 32, 32, 512)  0           conv3_block8_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block8_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block8_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 16, 16, 256)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_relu (Activation (None, 16, 16, 256)  0           conv4_block7_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block7_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_add (Add)          (None, 16, 16, 1024) 0           conv4_block6_out[0][0]           \n",
            "                                                                 conv4_block7_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_out (Activation)   (None, 16, 16, 1024) 0           conv4_block7_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block7_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 16, 16, 256)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_relu (Activation (None, 16, 16, 256)  0           conv4_block8_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block8_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_add (Add)          (None, 16, 16, 1024) 0           conv4_block7_out[0][0]           \n",
            "                                                                 conv4_block8_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_out (Activation)   (None, 16, 16, 1024) 0           conv4_block8_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block8_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 16, 16, 256)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_relu (Activation (None, 16, 16, 256)  0           conv4_block9_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block9_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_add (Add)          (None, 16, 16, 1024) 0           conv4_block8_out[0][0]           \n",
            "                                                                 conv4_block9_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_out (Activation)   (None, 16, 16, 1024) 0           conv4_block9_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block9_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block10_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block10_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_add (Add)         (None, 16, 16, 1024) 0           conv4_block9_out[0][0]           \n",
            "                                                                 conv4_block10_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_out (Activation)  (None, 16, 16, 1024) 0           conv4_block10_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block10_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block11_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block11_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_add (Add)         (None, 16, 16, 1024) 0           conv4_block10_out[0][0]          \n",
            "                                                                 conv4_block11_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_out (Activation)  (None, 16, 16, 1024) 0           conv4_block11_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block11_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block12_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block12_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_add (Add)         (None, 16, 16, 1024) 0           conv4_block11_out[0][0]          \n",
            "                                                                 conv4_block12_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_out (Activation)  (None, 16, 16, 1024) 0           conv4_block12_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block12_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block13_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block13_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_add (Add)         (None, 16, 16, 1024) 0           conv4_block12_out[0][0]          \n",
            "                                                                 conv4_block13_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_out (Activation)  (None, 16, 16, 1024) 0           conv4_block13_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block13_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block14_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block14_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_add (Add)         (None, 16, 16, 1024) 0           conv4_block13_out[0][0]          \n",
            "                                                                 conv4_block14_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_out (Activation)  (None, 16, 16, 1024) 0           conv4_block14_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block14_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block15_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block15_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_add (Add)         (None, 16, 16, 1024) 0           conv4_block14_out[0][0]          \n",
            "                                                                 conv4_block15_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_out (Activation)  (None, 16, 16, 1024) 0           conv4_block15_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block15_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block16_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block16_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_add (Add)         (None, 16, 16, 1024) 0           conv4_block15_out[0][0]          \n",
            "                                                                 conv4_block16_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_out (Activation)  (None, 16, 16, 1024) 0           conv4_block16_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block16_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block17_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block17_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_add (Add)         (None, 16, 16, 1024) 0           conv4_block16_out[0][0]          \n",
            "                                                                 conv4_block17_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_out (Activation)  (None, 16, 16, 1024) 0           conv4_block17_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block17_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block18_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block18_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_add (Add)         (None, 16, 16, 1024) 0           conv4_block17_out[0][0]          \n",
            "                                                                 conv4_block18_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_out (Activation)  (None, 16, 16, 1024) 0           conv4_block18_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block18_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block19_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block19_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_add (Add)         (None, 16, 16, 1024) 0           conv4_block18_out[0][0]          \n",
            "                                                                 conv4_block19_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_out (Activation)  (None, 16, 16, 1024) 0           conv4_block19_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block19_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block20_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block20_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_add (Add)         (None, 16, 16, 1024) 0           conv4_block19_out[0][0]          \n",
            "                                                                 conv4_block20_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_out (Activation)  (None, 16, 16, 1024) 0           conv4_block20_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block20_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block21_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block21_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_add (Add)         (None, 16, 16, 1024) 0           conv4_block20_out[0][0]          \n",
            "                                                                 conv4_block21_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_out (Activation)  (None, 16, 16, 1024) 0           conv4_block21_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block21_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block22_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block22_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_add (Add)         (None, 16, 16, 1024) 0           conv4_block21_out[0][0]          \n",
            "                                                                 conv4_block22_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_out (Activation)  (None, 16, 16, 1024) 0           conv4_block22_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block22_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block23_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block23_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_add (Add)         (None, 16, 16, 1024) 0           conv4_block22_out[0][0]          \n",
            "                                                                 conv4_block23_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_out (Activation)  (None, 16, 16, 1024) 0           conv4_block23_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block23_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block24_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block24_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block24_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_add (Add)         (None, 16, 16, 1024) 0           conv4_block23_out[0][0]          \n",
            "                                                                 conv4_block24_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_out (Activation)  (None, 16, 16, 1024) 0           conv4_block24_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block24_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block25_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block25_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block25_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block25_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block25_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block25_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block25_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_add (Add)         (None, 16, 16, 1024) 0           conv4_block24_out[0][0]          \n",
            "                                                                 conv4_block25_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_out (Activation)  (None, 16, 16, 1024) 0           conv4_block25_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block25_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block26_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block26_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block26_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block26_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block26_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block26_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block26_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_add (Add)         (None, 16, 16, 1024) 0           conv4_block25_out[0][0]          \n",
            "                                                                 conv4_block26_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_out (Activation)  (None, 16, 16, 1024) 0           conv4_block26_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block26_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block27_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block27_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block27_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block27_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block27_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block27_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block27_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_add (Add)         (None, 16, 16, 1024) 0           conv4_block26_out[0][0]          \n",
            "                                                                 conv4_block27_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_out (Activation)  (None, 16, 16, 1024) 0           conv4_block27_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block27_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block28_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block28_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block28_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block28_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block28_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block28_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block28_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_add (Add)         (None, 16, 16, 1024) 0           conv4_block27_out[0][0]          \n",
            "                                                                 conv4_block28_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_out (Activation)  (None, 16, 16, 1024) 0           conv4_block28_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block28_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block29_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block29_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block29_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block29_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block29_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block29_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block29_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_add (Add)         (None, 16, 16, 1024) 0           conv4_block28_out[0][0]          \n",
            "                                                                 conv4_block29_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_out (Activation)  (None, 16, 16, 1024) 0           conv4_block29_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block29_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block30_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block30_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block30_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block30_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block30_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block30_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block30_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_add (Add)         (None, 16, 16, 1024) 0           conv4_block29_out[0][0]          \n",
            "                                                                 conv4_block30_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_out (Activation)  (None, 16, 16, 1024) 0           conv4_block30_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block30_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block31_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block31_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block31_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block31_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block31_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block31_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block31_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_add (Add)         (None, 16, 16, 1024) 0           conv4_block30_out[0][0]          \n",
            "                                                                 conv4_block31_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_out (Activation)  (None, 16, 16, 1024) 0           conv4_block31_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block31_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block32_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block32_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block32_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block32_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block32_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block32_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block32_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_add (Add)         (None, 16, 16, 1024) 0           conv4_block31_out[0][0]          \n",
            "                                                                 conv4_block32_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_out (Activation)  (None, 16, 16, 1024) 0           conv4_block32_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block32_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block33_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block33_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block33_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block33_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block33_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block33_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block33_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_add (Add)         (None, 16, 16, 1024) 0           conv4_block32_out[0][0]          \n",
            "                                                                 conv4_block33_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_out (Activation)  (None, 16, 16, 1024) 0           conv4_block33_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block33_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block34_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block34_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block34_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block34_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block34_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block34_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block34_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_add (Add)         (None, 16, 16, 1024) 0           conv4_block33_out[0][0]          \n",
            "                                                                 conv4_block34_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_out (Activation)  (None, 16, 16, 1024) 0           conv4_block34_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block34_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block35_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block35_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block35_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block35_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block35_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block35_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block35_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_add (Add)         (None, 16, 16, 1024) 0           conv4_block34_out[0][0]          \n",
            "                                                                 conv4_block35_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_out (Activation)  (None, 16, 16, 1024) 0           conv4_block35_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block35_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block36_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block36_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block36_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block36_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block36_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block36_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block36_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_add (Add)         (None, 16, 16, 1024) 0           conv4_block35_out[0][0]          \n",
            "                                                                 conv4_block36_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_out (Activation)  (None, 16, 16, 1024) 0           conv4_block36_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block36_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block36_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_5 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 2)            4098        global_average_pooling2d_5[0][0] \n",
            "==================================================================================================\n",
            "Total params: 58,375,042\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 58,370,944\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IitOHvqs3VdZ"
      },
      "source": [
        "Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n06YIRCE3KCE",
        "outputId": "f3ab3914-edbd-48a8-ddc7-95e705994c44"
      },
      "source": [
        "base_model = keras.applications.Xception(weights='imagenet',include_top=False,input_shape=(img_height,img_width,3))\n",
        "\n",
        "# Set layers to non-trainable\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "# Add custom layers on top of Xception\n",
        "global_avg_pooling = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(num_classes,activation='sigmoid')(global_avg_pooling)\n",
        "\n",
        "Xception = keras.models.Model(inputs=base_model.input,outputs=output,name='Xception')\n",
        "Xception.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 250, 250, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 124, 124, 32) 864         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 124, 124, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 124, 124, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 122, 122, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 122, 122, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 122, 122, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 122, 122, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 122, 122, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 122, 122, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 122, 122, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 122, 122, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 61, 61, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 61, 61, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 61, 61, 128)  512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 61, 61, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 61, 61, 128)  0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 61, 61, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 61, 61, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 61, 61, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 61, 61, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 61, 61, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 31, 31, 256)  32768       add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 31, 31, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 31, 31, 256)  1024        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 31, 31, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 31, 31, 256)  0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 31, 31, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 31, 31, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 31, 31, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 31, 31, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 31, 31, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 728)  186368      add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 16, 16, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 728)  2912        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 16, 16, 728)  0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 16, 16, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 16, 16, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 16, 16, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 16, 16, 728)  0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 16, 16, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 16, 16, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 16, 16, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 16, 16, 728)  0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 16, 16, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 16, 16, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 16, 16, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 16, 16, 728)  0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 16, 16, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 16, 16, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 16, 16, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 16, 16, 728)  0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 16, 16, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 16, 16, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 16, 16, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 16, 16, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 16, 16, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 16, 16, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 16, 16, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 16, 16, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 16, 16, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 16, 16, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 16, 16, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 16, 16, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 16, 16, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 16, 16, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 16, 16, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 16, 16, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 16, 16, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 16, 16, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 16, 16, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 16, 16, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 16, 16, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 16, 16, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 16, 16, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 16, 16, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 16, 16, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 8, 8, 1024)   745472      add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 8, 8, 1024)   0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 8, 8, 1024)   4096        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 8, 8, 1024)   0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 8, 8, 1536)   1582080     add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 8, 8, 1536)   6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 8, 8, 1536)   0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 8, 8, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 8, 8, 2048)   8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 8, 8, 2048)   0           block14_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_6 (Glo (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 2)            4098        global_average_pooling2d_6[0][0] \n",
            "==================================================================================================\n",
            "Total params: 20,865,578\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 20,861,480\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxpwYfBs4aaq"
      },
      "source": [
        "MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8VCadAS4KSa",
        "outputId": "72a65b27-2d7b-4f61-eb26-2f475fb439b3"
      },
      "source": [
        "base_model = keras.applications.MobileNet(weights='imagenet',include_top=False,input_shape=(img_height,img_width,3))\n",
        "\n",
        "# set layers to non-trainable\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "# Adding custom layer on top of MobileNet\n",
        "global_avg_pooling = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(num_classes,activation='sigmoid')(global_avg_pooling)\n",
        "\n",
        "MobileNet = keras.models.Model(inputs=base_model.input,outputs=output,name='MobileNet')\n",
        "MobileNet.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Model: \"MobileNet\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 250, 250, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 125, 125, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 125, 125, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 125, 125, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 125, 125, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 125, 125, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 125, 125, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 125, 125, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 125, 125, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 125, 125, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 126, 126, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 62, 62, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 62, 62, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 62, 62, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 62, 62, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 62, 62, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 62, 62, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 62, 62, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 62, 62, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 62, 62, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 31, 31, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 31, 31, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 31, 31, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 31, 31, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 31, 31, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 15, 15, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 15, 15, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 15, 15, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 15, 15, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_7 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 3,230,914\n",
            "Trainable params: 2,050\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE0sx42X5hxK"
      },
      "source": [
        "Training\n",
        "\n",
        "1. VGG16\n",
        "2. ResNet50\n",
        "3. ResNet152\n",
        "4. Xception\n",
        "5. MobileNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EehpdqkCtPCX",
        "outputId": "135f4adb-fae9-4be1-e29a-7ca705b0156b"
      },
      "source": [
        "face_classifier = MobileNet\n",
        "face_classifier.summary()\n",
        "\n",
        "name_to_save = f\"models/face_classifier_{face_classifier.name}_aug.h5\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"MobileNet\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 250, 250, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 125, 125, 32)      864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, 125, 125, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, 125, 125, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, 125, 125, 32)      288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, 125, 125, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, 125, 125, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, 125, 125, 64)      2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, 125, 125, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, 125, 125, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, 126, 126, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, 62, 62, 64)        576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, 62, 62, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, 62, 62, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, 62, 62, 128)       8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, 62, 62, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, 62, 62, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, 62, 62, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, 62, 62, 128)       16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, 62, 62, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, 63, 63, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, 31, 31, 128)       1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, 31, 31, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, 31, 31, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, 31, 31, 256)       32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, 31, 31, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, 31, 31, 256)       65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, 31, 31, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, 15, 15, 256)       2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, 15, 15, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, 15, 15, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, 15, 15, 512)       131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, 15, 15, 512)       4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, 15, 15, 512)       262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, 15, 15, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_7 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 3,230,914\n",
            "Trainable params: 2,050\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaFS11JLtnLp"
      },
      "source": [
        "# ModelCheckpoint to save model in case of interrupting the learning process\n",
        "checkpoint = ModelCheckpoint(name_to_save,\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only=True,\n",
        "                             verbose=1)\n",
        "\n",
        "# EarlyStopping to find best model with a large number of epochs\n",
        "earlystop = EarlyStopping(monitor='val_loss',\n",
        "                          restore_best_weights=True,\n",
        "                          patience=100,\n",
        "                          verbose=1)\n",
        "callbacks=[earlystop,checkpoint]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twBSks3fugav"
      },
      "source": [
        "face_classifier.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=0.1),metrics=['accuracy'])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F62Hfwg2uvqO"
      },
      "source": [
        "epochs=500"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvXRF-XPuxhs",
        "outputId": "8738bc94-9e84-4260-9659-2c0ad582cbd0"
      },
      "source": [
        "history=face_classifier.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds)\n",
        "face_classifier.save(name_to_save)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "5/5 [==============================] - 2s 153ms/step - loss: 4.4477 - accuracy: 0.7653 - val_loss: 0.5719 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.57191, saving model to models/face_classifier_MobileNet_aug.h5\n",
            "Epoch 2/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 1.8255 - accuracy: 0.9184 - val_loss: 5.9524 - val_accuracy: 0.7917\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.57191\n",
            "Epoch 3/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 1.4205 - accuracy: 0.9184 - val_loss: 1.6079 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.57191\n",
            "Epoch 4/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.4589 - accuracy: 0.9286 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.57191 to 0.02976, saving model to models/face_classifier_MobileNet_aug.h5\n",
            "Epoch 5/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0377 - accuracy: 0.9898 - val_loss: 1.9773 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.02976\n",
            "Epoch 6/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0409 - accuracy: 0.9796 - val_loss: 0.7815 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02976\n",
            "Epoch 7/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.1480 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.02976\n",
            "Epoch 8/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 2.4328e-09 - accuracy: 1.0000 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.02976 to 0.00413, saving model to models/face_classifier_MobileNet_aug.h5\n",
            "Epoch 9/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 2.6746e-06 - accuracy: 1.0000 - val_loss: 0.1608 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00413\n",
            "Epoch 10/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 3.1423e-05 - accuracy: 1.0000 - val_loss: 0.3273 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00413\n",
            "Epoch 11/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.2573 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00413\n",
            "Epoch 12/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 9.2385e-06 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00413\n",
            "Epoch 13/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 7.1769e-08 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00413 to 0.00245, saving model to models/face_classifier_MobileNet_aug.h5\n",
            "Epoch 14/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 6.0821e-09 - accuracy: 1.0000 - val_loss: 5.3537e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00245 to 0.00054, saving model to models/face_classifier_MobileNet_aug.h5\n",
            "Epoch 15/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 1.2164e-09 - accuracy: 1.0000 - val_loss: 7.5465e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00054\n",
            "Epoch 16/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00054\n",
            "Epoch 17/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00054\n",
            "Epoch 18/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00054\n",
            "Epoch 19/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00054\n",
            "Epoch 20/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00054\n",
            "Epoch 21/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00054\n",
            "Epoch 22/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00054\n",
            "Epoch 23/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00054\n",
            "Epoch 24/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00054\n",
            "Epoch 25/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00054\n",
            "Epoch 26/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00054\n",
            "Epoch 27/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00054\n",
            "Epoch 28/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00054\n",
            "Epoch 29/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00054\n",
            "Epoch 30/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00054\n",
            "Epoch 31/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00054\n",
            "Epoch 32/500\n",
            "5/5 [==============================] - 1s 58ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00054\n",
            "Epoch 33/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00054\n",
            "Epoch 34/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00054\n",
            "Epoch 35/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00054\n",
            "Epoch 36/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00054\n",
            "Epoch 37/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00054\n",
            "Epoch 38/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00054\n",
            "Epoch 39/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00054\n",
            "Epoch 40/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00054\n",
            "Epoch 41/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00054\n",
            "Epoch 42/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00054\n",
            "Epoch 43/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00054\n",
            "Epoch 44/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00054\n",
            "Epoch 45/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00054\n",
            "Epoch 46/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00054\n",
            "Epoch 47/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00054\n",
            "Epoch 48/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00054\n",
            "Epoch 49/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00054\n",
            "Epoch 50/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00054\n",
            "Epoch 51/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00054\n",
            "Epoch 52/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00054\n",
            "Epoch 53/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00054\n",
            "Epoch 54/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00054\n",
            "Epoch 55/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00054\n",
            "Epoch 56/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00054\n",
            "Epoch 57/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00054\n",
            "Epoch 58/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00054\n",
            "Epoch 59/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00054\n",
            "Epoch 60/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00054\n",
            "Epoch 61/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00054\n",
            "Epoch 62/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00054\n",
            "Epoch 63/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00054\n",
            "Epoch 64/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00054\n",
            "Epoch 65/500\n",
            "5/5 [==============================] - 1s 57ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00054\n",
            "Epoch 66/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00054\n",
            "Epoch 67/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00054\n",
            "Epoch 68/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00054\n",
            "Epoch 69/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00054\n",
            "Epoch 70/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00054\n",
            "Epoch 71/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00054\n",
            "Epoch 72/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00054\n",
            "Epoch 73/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00054\n",
            "Epoch 74/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00054\n",
            "Epoch 75/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00054\n",
            "Epoch 76/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00054\n",
            "Epoch 77/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00054\n",
            "Epoch 78/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00054\n",
            "Epoch 79/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00054\n",
            "Epoch 80/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00054\n",
            "Epoch 81/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00054\n",
            "Epoch 82/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00054\n",
            "Epoch 83/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00054\n",
            "Epoch 84/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00054\n",
            "Epoch 85/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00054\n",
            "Epoch 86/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00054\n",
            "Epoch 87/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00054\n",
            "Epoch 88/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00054\n",
            "Epoch 89/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00054\n",
            "Epoch 90/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00054\n",
            "Epoch 91/500\n",
            "5/5 [==============================] - 1s 57ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00054\n",
            "Epoch 92/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00054\n",
            "Epoch 93/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00054\n",
            "Epoch 94/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00054\n",
            "Epoch 95/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00054\n",
            "Epoch 96/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00054\n",
            "Epoch 97/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00054\n",
            "Epoch 98/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00054\n",
            "Epoch 99/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00054\n",
            "Epoch 100/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00054\n",
            "Epoch 101/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00054\n",
            "Epoch 102/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00054\n",
            "Epoch 103/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.00054\n",
            "Epoch 104/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.00054\n",
            "Epoch 105/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.00054\n",
            "Epoch 106/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.00054\n",
            "Epoch 107/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.00054\n",
            "Epoch 108/500\n",
            "5/5 [==============================] - 1s 51ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.00054\n",
            "Epoch 109/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.00054\n",
            "Epoch 110/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.00054\n",
            "Epoch 111/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.00054\n",
            "Epoch 112/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.00054\n",
            "Epoch 113/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.00054\n",
            "Epoch 114/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.00054\n",
            "Epoch 115/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.00054\n",
            "Epoch 116/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.00054\n",
            "Epoch 117/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.00054\n",
            "Epoch 118/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.00054\n",
            "Epoch 119/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.00054\n",
            "Epoch 120/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.00054\n",
            "Epoch 121/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.00054\n",
            "Epoch 122/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.00054\n",
            "Epoch 123/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.00054\n",
            "Epoch 124/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.00054\n",
            "Epoch 125/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.00054\n",
            "Epoch 126/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.00054\n",
            "Epoch 127/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.00054\n",
            "Epoch 128/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.00054\n",
            "Epoch 129/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.00054\n",
            "Epoch 130/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.00054\n",
            "Epoch 131/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.00054\n",
            "Epoch 132/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.00054\n",
            "Epoch 133/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.00054\n",
            "Epoch 134/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.00054\n",
            "Epoch 135/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.00054\n",
            "Epoch 136/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.00054\n",
            "Epoch 137/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.00054\n",
            "Epoch 138/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.00054\n",
            "Epoch 139/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.00054\n",
            "Epoch 140/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.00054\n",
            "Epoch 141/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.00054\n",
            "Epoch 142/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.00054\n",
            "Epoch 143/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.00054\n",
            "Epoch 144/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.00054\n",
            "Epoch 145/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.00054\n",
            "Epoch 146/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.00054\n",
            "Epoch 147/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.00054\n",
            "Epoch 148/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.00054\n",
            "Epoch 149/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00054\n",
            "Epoch 150/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00054\n",
            "Epoch 151/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00054\n",
            "Epoch 152/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00054\n",
            "Epoch 153/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00054\n",
            "Epoch 154/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00054\n",
            "Epoch 155/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00054\n",
            "Epoch 156/500\n",
            "5/5 [==============================] - 1s 57ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00054\n",
            "Epoch 157/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00054\n",
            "Epoch 158/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00054\n",
            "Epoch 159/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00054\n",
            "Epoch 160/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00054\n",
            "Epoch 161/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00054\n",
            "Epoch 162/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00054\n",
            "Epoch 163/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00054\n",
            "Epoch 164/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00054\n",
            "Epoch 165/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.00054\n",
            "Epoch 166/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00054\n",
            "Epoch 167/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00054\n",
            "Epoch 168/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00054\n",
            "Epoch 169/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00054\n",
            "Epoch 170/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00054\n",
            "Epoch 171/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00054\n",
            "Epoch 172/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00054\n",
            "Epoch 173/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00054\n",
            "Epoch 174/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00054\n",
            "Epoch 175/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00054\n",
            "Epoch 176/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00054\n",
            "Epoch 177/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00054\n",
            "Epoch 178/500\n",
            "5/5 [==============================] - 1s 51ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00054\n",
            "Epoch 179/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00054\n",
            "Epoch 180/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00054\n",
            "Epoch 181/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00054\n",
            "Epoch 182/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00054\n",
            "Epoch 183/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00054\n",
            "Epoch 184/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00054\n",
            "Epoch 185/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00054\n",
            "Epoch 186/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00054\n",
            "Epoch 187/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00054\n",
            "Epoch 188/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00054\n",
            "Epoch 189/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00054\n",
            "Epoch 190/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00054\n",
            "Epoch 191/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00054\n",
            "Epoch 192/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00054\n",
            "Epoch 193/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00054\n",
            "Epoch 194/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00054\n",
            "Epoch 195/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00054\n",
            "Epoch 196/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00054\n",
            "Epoch 197/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00054\n",
            "Epoch 198/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00054\n",
            "Epoch 199/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00054\n",
            "Epoch 200/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00054\n",
            "Epoch 201/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.00054\n",
            "Epoch 202/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.00054\n",
            "Epoch 203/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.00054\n",
            "Epoch 204/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.00054\n",
            "Epoch 205/500\n",
            "5/5 [==============================] - 1s 56ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.00054\n",
            "Epoch 206/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.00054\n",
            "Epoch 207/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.00054\n",
            "Epoch 208/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.00054\n",
            "Epoch 209/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.00054\n",
            "Epoch 210/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.00054\n",
            "Epoch 211/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.00054\n",
            "Epoch 212/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.00054\n",
            "Epoch 213/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.00054\n",
            "Epoch 214/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.00054\n",
            "Epoch 215/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.00054\n",
            "Epoch 216/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.00054\n",
            "Epoch 217/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.00054\n",
            "Epoch 218/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.00054\n",
            "Epoch 219/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.00054\n",
            "Epoch 220/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.00054\n",
            "Epoch 221/500\n",
            "5/5 [==============================] - 1s 58ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.00054\n",
            "Epoch 222/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.00054\n",
            "Epoch 223/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.00054\n",
            "Epoch 224/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.00054\n",
            "Epoch 225/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.00054\n",
            "Epoch 226/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.00054\n",
            "Epoch 227/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.00054\n",
            "Epoch 228/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.00054\n",
            "Epoch 229/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.00054\n",
            "Epoch 230/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.00054\n",
            "Epoch 231/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.00054\n",
            "Epoch 232/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.00054\n",
            "Epoch 233/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.00054\n",
            "Epoch 234/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.00054\n",
            "Epoch 235/500\n",
            "5/5 [==============================] - 1s 57ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.00054\n",
            "Epoch 236/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.00054\n",
            "Epoch 237/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.00054\n",
            "Epoch 238/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.00054\n",
            "Epoch 239/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.00054\n",
            "Epoch 240/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.00054\n",
            "Epoch 241/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.00054\n",
            "Epoch 242/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.00054\n",
            "Epoch 243/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.00054\n",
            "Epoch 244/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.00054\n",
            "Epoch 245/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.00054\n",
            "Epoch 246/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.00054\n",
            "Epoch 247/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.00054\n",
            "Epoch 248/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.00054\n",
            "Epoch 249/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.00054\n",
            "Epoch 250/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.00054\n",
            "Epoch 251/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.00054\n",
            "Epoch 252/500\n",
            "5/5 [==============================] - 1s 55ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.00054\n",
            "Epoch 253/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.00054\n",
            "Epoch 254/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.00054\n",
            "Epoch 255/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.00054\n",
            "Epoch 256/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.00054\n",
            "Epoch 257/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.00054\n",
            "Epoch 258/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.00054\n",
            "Epoch 259/500\n",
            "5/5 [==============================] - 1s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.00054\n",
            "Epoch 260/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.00054\n",
            "Epoch 261/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.00054\n",
            "Epoch 262/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.00054\n",
            "Epoch 263/500\n",
            "5/5 [==============================] - 1s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.00054\n",
            "Epoch 264/500\n",
            "5/5 [==============================] - 1s 54ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.00054\n",
            "Epoch 00264: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noKQXDl-vEsD"
      },
      "source": [
        "model_name = 'face_classifier_MobileNet_aug.h5'\n",
        "face_classifier = keras.models.load_model(f'models/{model_name}')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C9Ae1020pWm"
      },
      "source": [
        "def test_image_classifier_with_folder(model,path,y_true,img_height=250,img_width=250,class_names=['me','not_me']):\n",
        "  num_classes=len(class_names)\n",
        "  total=0\n",
        "  correct=0\n",
        "\n",
        "  for filename in os.listdir(path):\n",
        "    test_path = os.path.join(path,filename)\n",
        "    test_image = keras.preprocessing.image.load_img(test_path,target_size=(img_height,img_width,3))\n",
        "    test_image = keras.preprocessing.image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image,axis=0)\n",
        "    result = model.predict(test_image)\n",
        "\n",
        "    y_pred = class_names[np.array(result[0]).argmax(axis=0)]\n",
        "    iscorrect = 'correct' if y_pred == y_true else 'incorrect'\n",
        "    print('{}-{}'.format(iscorrect,filename))\n",
        "    for index in range(num_classes):\n",
        "      print(\"\\t{:6} with probability of {:.2f}%\".format(class_names[index],result[0][index]*100))\n",
        "    total+=1\n",
        "    if y_pred == y_true:\n",
        "      correct+=1\n",
        "  print(\"\\nTotal accuracy is {:.2f}%={}/{} samples classified correctly\".format(correct/total*100,correct,total))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE3cSpgs2nSb",
        "outputId": "65adeb2f-45ea-4be7-9b91-277c6afe5085"
      },
      "source": [
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/me',y_true='me')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct-IMG_0004.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004113291.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-18f3ca8e-f3d6-4020-812b-1800b5b67c15~2.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-FB_IMG_1543004107327.jpg\n",
            "\tme     with probability of 0.71%\n",
            "\tnot_me with probability of 99.70%\n",
            "correct-a8e5e4f3-4b4b-4d22-ade3-1acd6c74bc10.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004173806.jpg\n",
            "\tme     with probability of 63.94%\n",
            "\tnot_me with probability of 20.53%\n",
            "correct-FB_IMG_1543004123855.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-IMG_0077.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-IMG_0007.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "\n",
            "Total accuracy is 77.78%=7/9 samples classified correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5K6fe1R20wb",
        "outputId": "4e68a648-deb0-4b4c-e341-de3ea40e4bf2"
      },
      "source": [
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/not_me',y_true='not_me')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct-Aicha_El_Ouafi_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Alan_Ball_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aicha_El_Ouafi_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aitor_Gonzalez_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0003.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "incorrect-Aitor_Gonzalez_0001.jpg\n",
            "\tme     with probability of 75.57%\n",
            "\tnot_me with probability of 11.88%\n",
            "incorrect-Aicha_El_Ouafi_0003.jpg\n",
            "\tme     with probability of 78.73%\n",
            "\tnot_me with probability of 49.89%\n",
            "\n",
            "Total accuracy is 77.78%=7/9 samples classified correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbEx601f3pgz",
        "outputId": "374e37fa-55db-40ee-e1a5-49e1c5e554c4"
      },
      "source": [
        "face_classifier = Xception\n",
        "face_classifier.summary()\n",
        "\n",
        "name_to_save = f\"models/face_classifier_{face_classifier.name}_aug.h5\"\n",
        "\n",
        "face_classifier.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=0.1),metrics=['accuracy'])\n",
        "epochs=500\n",
        "history=face_classifier.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds)\n",
        "face_classifier.save(name_to_save)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Xception\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 250, 250, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 124, 124, 32) 864         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 124, 124, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 124, 124, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 122, 122, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 122, 122, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 122, 122, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 122, 122, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 122, 122, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 122, 122, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 122, 122, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 122, 122, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 61, 61, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 61, 61, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 61, 61, 128)  512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 61, 61, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 61, 61, 128)  0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 61, 61, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 61, 61, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 61, 61, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 61, 61, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 61, 61, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 31, 31, 256)  32768       add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 31, 31, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 31, 31, 256)  1024        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 31, 31, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 31, 31, 256)  0           add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 31, 31, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 31, 31, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 31, 31, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 31, 31, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 31, 31, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 728)  186368      add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 16, 16, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 728)  2912        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 16, 16, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 16, 16, 728)  0           add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 16, 16, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 16, 16, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 16, 16, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 16, 16, 728)  0           add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 16, 16, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 16, 16, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 16, 16, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 16, 16, 728)  0           add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 16, 16, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 16, 16, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 16, 16, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 16, 16, 728)  0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 16, 16, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 16, 16, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 16, 16, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 16, 16, 728)  0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 16, 16, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 16, 16, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 16, 16, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 16, 16, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 16, 16, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 16, 16, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 16, 16, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 16, 16, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 16, 16, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 16, 16, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 16, 16, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 16, 16, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 16, 16, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 16, 16, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 16, 16, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 16, 16, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 16, 16, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 16, 16, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 16, 16, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 16, 16, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 16, 16, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 16, 16, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 16, 16, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 16, 16, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 16, 16, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 16, 16, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 16, 16, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 16, 16, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 16, 16, 728)  0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 16, 16, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 16, 16, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 16, 16, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 16, 16, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 16, 16, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 8, 8, 1024)   745472      add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 8, 8, 1024)   0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 8, 8, 1024)   4096        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 8, 8, 1024)   0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 8, 8, 1536)   1582080     add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 8, 8, 1536)   6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 8, 8, 1536)   0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 8, 8, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 8, 8, 2048)   8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 8, 8, 2048)   0           block14_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_6 (Glo (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 2)            4098        global_average_pooling2d_6[0][0] \n",
            "==================================================================================================\n",
            "Total params: 20,865,578\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 20,861,480\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "5/5 [==============================] - 35s 358ms/step - loss: 194.7944 - accuracy: 0.5816 - val_loss: 321.1154 - val_accuracy: 0.4167\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00054\n",
            "Epoch 2/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 148.4013 - accuracy: 0.5000 - val_loss: 87.2331 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00054\n",
            "Epoch 3/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 77.7533 - accuracy: 0.7449 - val_loss: 276.7553 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00054\n",
            "Epoch 4/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 77.4082 - accuracy: 0.7449 - val_loss: 75.2054 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00054\n",
            "Epoch 5/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 51.8496 - accuracy: 0.7653 - val_loss: 59.6157 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00054\n",
            "Epoch 6/500\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 41.4214 - accuracy: 0.8469 - val_loss: 183.6897 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00054\n",
            "Epoch 7/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 28.8619 - accuracy: 0.8163 - val_loss: 42.0229 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00054\n",
            "Epoch 8/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 19.9920 - accuracy: 0.8776 - val_loss: 72.3630 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00054\n",
            "Epoch 9/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 7.3901 - accuracy: 0.9082 - val_loss: 55.0043 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00054\n",
            "Epoch 10/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 1.9573 - accuracy: 0.9592 - val_loss: 44.4826 - val_accuracy: 0.7917\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00054\n",
            "Epoch 11/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.7608 - accuracy: 0.9796 - val_loss: 58.2881 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00054\n",
            "Epoch 12/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 2.1808 - accuracy: 0.9796 - val_loss: 55.4632 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00054\n",
            "Epoch 13/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.4779 - accuracy: 0.9796 - val_loss: 32.1579 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00054\n",
            "Epoch 14/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 1.7869 - accuracy: 0.9694 - val_loss: 63.8227 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00054\n",
            "Epoch 15/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 4.3329 - accuracy: 0.9490 - val_loss: 39.9449 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00054\n",
            "Epoch 16/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.5572 - accuracy: 0.9796 - val_loss: 26.5240 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00054\n",
            "Epoch 17/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.5392 - accuracy: 0.9694 - val_loss: 41.9330 - val_accuracy: 0.7917\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00054\n",
            "Epoch 18/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 1.9605 - accuracy: 0.9796 - val_loss: 44.1878 - val_accuracy: 0.7917\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00054\n",
            "Epoch 19/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 1.2164e-09 - accuracy: 1.0000 - val_loss: 27.4607 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00054\n",
            "Epoch 20/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.1284 - accuracy: 0.9898 - val_loss: 50.7592 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00054\n",
            "Epoch 21/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 80.0716 - val_accuracy: 0.7500\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00054\n",
            "Epoch 22/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 1.5865 - accuracy: 0.9796 - val_loss: 44.9186 - val_accuracy: 0.7917\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00054\n",
            "Epoch 23/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.3552 - accuracy: 0.9796 - val_loss: 26.5390 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00054\n",
            "Epoch 24/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.1826 - accuracy: 0.9898 - val_loss: 29.5889 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00054\n",
            "Epoch 25/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 36.3531 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00054\n",
            "Epoch 26/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0230 - accuracy: 0.9796 - val_loss: 36.0651 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00054\n",
            "Epoch 27/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 34.0477 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00054\n",
            "Epoch 28/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 32.8327 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00054\n",
            "Epoch 29/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 32.1014 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00054\n",
            "Epoch 30/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.6621 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00054\n",
            "Epoch 31/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.4133 - val_accuracy: 0.8333\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00054\n",
            "Epoch 32/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.3048 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00054\n",
            "Epoch 33/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.2559 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00054\n",
            "Epoch 34/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.2289 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00054\n",
            "Epoch 35/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.2131 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00054\n",
            "Epoch 36/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.2038 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00054\n",
            "Epoch 37/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1982 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00054\n",
            "Epoch 38/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1948 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00054\n",
            "Epoch 39/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1928 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00054\n",
            "Epoch 40/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1917 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00054\n",
            "Epoch 41/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1909 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00054\n",
            "Epoch 42/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1905 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00054\n",
            "Epoch 43/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1902 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00054\n",
            "Epoch 44/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1901 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00054\n",
            "Epoch 45/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1900 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00054\n",
            "Epoch 46/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00054\n",
            "Epoch 47/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00054\n",
            "Epoch 48/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00054\n",
            "Epoch 49/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00054\n",
            "Epoch 50/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00054\n",
            "Epoch 51/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00054\n",
            "Epoch 52/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00054\n",
            "Epoch 53/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00054\n",
            "Epoch 54/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00054\n",
            "Epoch 55/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00054\n",
            "Epoch 56/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00054\n",
            "Epoch 57/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00054\n",
            "Epoch 58/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00054\n",
            "Epoch 59/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00054\n",
            "Epoch 60/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00054\n",
            "Epoch 61/500\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00054\n",
            "Epoch 62/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00054\n",
            "Epoch 63/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00054\n",
            "Epoch 64/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00054\n",
            "Epoch 65/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00054\n",
            "Epoch 66/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00054\n",
            "Epoch 67/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00054\n",
            "Epoch 68/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00054\n",
            "Epoch 69/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00054\n",
            "Epoch 70/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00054\n",
            "Epoch 71/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00054\n",
            "Epoch 72/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00054\n",
            "Epoch 73/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00054\n",
            "Epoch 74/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00054\n",
            "Epoch 75/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00054\n",
            "Epoch 76/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00054\n",
            "Epoch 77/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00054\n",
            "Epoch 78/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00054\n",
            "Epoch 79/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00054\n",
            "Epoch 80/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00054\n",
            "Epoch 81/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00054\n",
            "Epoch 82/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00054\n",
            "Epoch 83/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00054\n",
            "Epoch 84/500\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00054\n",
            "Epoch 85/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00054\n",
            "Epoch 86/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00054\n",
            "Epoch 87/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00054\n",
            "Epoch 88/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00054\n",
            "Epoch 89/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00054\n",
            "Epoch 90/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00054\n",
            "Epoch 91/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00054\n",
            "Epoch 92/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00054\n",
            "Epoch 93/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00054\n",
            "Epoch 94/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00054\n",
            "Epoch 95/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00054\n",
            "Epoch 96/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00054\n",
            "Epoch 97/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00054\n",
            "Epoch 98/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00054\n",
            "Epoch 99/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00054\n",
            "Epoch 100/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00054\n",
            "Epoch 101/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00054\n",
            "Epoch 102/500\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00054\n",
            "Epoch 103/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.00054\n",
            "Epoch 104/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.00054\n",
            "Epoch 105/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.00054\n",
            "Epoch 106/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.00054\n",
            "Epoch 107/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.00054\n",
            "Epoch 108/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.00054\n",
            "Epoch 109/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.00054\n",
            "Epoch 110/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.00054\n",
            "Epoch 111/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.00054\n",
            "Epoch 112/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.00054\n",
            "Epoch 113/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.00054\n",
            "Epoch 114/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.00054\n",
            "Epoch 115/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.00054\n",
            "Epoch 116/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.00054\n",
            "Epoch 117/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.00054\n",
            "Epoch 118/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.00054\n",
            "Epoch 119/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.00054\n",
            "Epoch 120/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.00054\n",
            "Epoch 121/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.00054\n",
            "Epoch 122/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.00054\n",
            "Epoch 123/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.00054\n",
            "Epoch 124/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.00054\n",
            "Epoch 125/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.00054\n",
            "Epoch 126/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.00054\n",
            "Epoch 127/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.00054\n",
            "Epoch 128/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.00054\n",
            "Epoch 129/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.00054\n",
            "Epoch 130/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.00054\n",
            "Epoch 131/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.00054\n",
            "Epoch 132/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.00054\n",
            "Epoch 133/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.00054\n",
            "Epoch 134/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.00054\n",
            "Epoch 135/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.00054\n",
            "Epoch 136/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.00054\n",
            "Epoch 137/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.00054\n",
            "Epoch 138/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.00054\n",
            "Epoch 139/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.00054\n",
            "Epoch 140/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.00054\n",
            "Epoch 141/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.00054\n",
            "Epoch 142/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.00054\n",
            "Epoch 143/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.00054\n",
            "Epoch 144/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.00054\n",
            "Epoch 145/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.00054\n",
            "Epoch 146/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.00054\n",
            "Epoch 147/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.00054\n",
            "Epoch 148/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.00054\n",
            "Epoch 149/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00054\n",
            "Epoch 150/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00054\n",
            "Epoch 151/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00054\n",
            "Epoch 152/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00054\n",
            "Epoch 153/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00054\n",
            "Epoch 154/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00054\n",
            "Epoch 155/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00054\n",
            "Epoch 156/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00054\n",
            "Epoch 157/500\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00054\n",
            "Epoch 158/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00054\n",
            "Epoch 159/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00054\n",
            "Epoch 160/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00054\n",
            "Epoch 161/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00054\n",
            "Epoch 162/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00054\n",
            "Epoch 163/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00054\n",
            "Epoch 164/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00054\n",
            "Epoch 165/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.00054\n",
            "Epoch 166/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00054\n",
            "Epoch 167/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00054\n",
            "Epoch 168/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00054\n",
            "Epoch 169/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00054\n",
            "Epoch 170/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00054\n",
            "Epoch 171/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00054\n",
            "Epoch 172/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00054\n",
            "Epoch 173/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00054\n",
            "Epoch 174/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00054\n",
            "Epoch 175/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00054\n",
            "Epoch 176/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00054\n",
            "Epoch 177/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00054\n",
            "Epoch 178/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00054\n",
            "Epoch 179/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00054\n",
            "Epoch 180/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00054\n",
            "Epoch 181/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00054\n",
            "Epoch 182/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00054\n",
            "Epoch 183/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00054\n",
            "Epoch 184/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00054\n",
            "Epoch 185/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00054\n",
            "Epoch 186/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00054\n",
            "Epoch 187/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00054\n",
            "Epoch 188/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00054\n",
            "Epoch 189/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00054\n",
            "Epoch 190/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00054\n",
            "Epoch 191/500\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00054\n",
            "Epoch 192/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00054\n",
            "Epoch 193/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00054\n",
            "Epoch 194/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00054\n",
            "Epoch 195/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00054\n",
            "Epoch 196/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00054\n",
            "Epoch 197/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00054\n",
            "Epoch 198/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00054\n",
            "Epoch 199/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00054\n",
            "Epoch 200/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00054\n",
            "Epoch 201/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.00054\n",
            "Epoch 202/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.00054\n",
            "Epoch 203/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.00054\n",
            "Epoch 204/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.00054\n",
            "Epoch 205/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.00054\n",
            "Epoch 206/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.00054\n",
            "Epoch 207/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.00054\n",
            "Epoch 208/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.00054\n",
            "Epoch 209/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.00054\n",
            "Epoch 210/500\n",
            "5/5 [==============================] - 1s 108ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.00054\n",
            "Epoch 211/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.00054\n",
            "Epoch 212/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.00054\n",
            "Epoch 213/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.00054\n",
            "Epoch 214/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.00054\n",
            "Epoch 215/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.00054\n",
            "Epoch 216/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.00054\n",
            "Epoch 217/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.00054\n",
            "Epoch 218/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.00054\n",
            "Epoch 219/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.00054\n",
            "Epoch 220/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.00054\n",
            "Epoch 221/500\n",
            "5/5 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.00054\n",
            "Epoch 222/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.00054\n",
            "Epoch 223/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.00054\n",
            "Epoch 224/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.00054\n",
            "Epoch 225/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.00054\n",
            "Epoch 226/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.00054\n",
            "Epoch 227/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.00054\n",
            "Epoch 228/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.00054\n",
            "Epoch 229/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.00054\n",
            "Epoch 230/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.00054\n",
            "Epoch 231/500\n",
            "5/5 [==============================] - 1s 112ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.00054\n",
            "Epoch 232/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.00054\n",
            "Epoch 233/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.00054\n",
            "Epoch 234/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.00054\n",
            "Epoch 235/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.00054\n",
            "Epoch 236/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.00054\n",
            "Epoch 237/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.00054\n",
            "Epoch 238/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.00054\n",
            "Epoch 239/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.00054\n",
            "Epoch 240/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.00054\n",
            "Epoch 241/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.00054\n",
            "Epoch 242/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.00054\n",
            "Epoch 243/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.00054\n",
            "Epoch 244/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.00054\n",
            "Epoch 245/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.00054\n",
            "Epoch 246/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.00054\n",
            "Epoch 247/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.00054\n",
            "Epoch 248/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.00054\n",
            "Epoch 249/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.00054\n",
            "Epoch 250/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.00054\n",
            "Epoch 251/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.00054\n",
            "Epoch 252/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.00054\n",
            "Epoch 253/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.00054\n",
            "Epoch 254/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.00054\n",
            "Epoch 255/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.00054\n",
            "Epoch 256/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.00054\n",
            "Epoch 257/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.00054\n",
            "Epoch 258/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.00054\n",
            "Epoch 259/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.00054\n",
            "Epoch 260/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.00054\n",
            "Epoch 261/500\n",
            "5/5 [==============================] - 1s 110ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.00054\n",
            "Epoch 262/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.00054\n",
            "Epoch 263/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.00054\n",
            "Epoch 264/500\n",
            "5/5 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.00054\n",
            "Epoch 265/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.00054\n",
            "Epoch 266/500\n",
            "5/5 [==============================] - 1s 109ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 31.1899 - val_accuracy: 0.8750\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.00054\n",
            "Epoch 00266: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlQPln_r4g6n",
        "outputId": "6772dd85-4e0d-4bba-a467-b32ebca3f2ae"
      },
      "source": [
        "model_name = 'face_classifier_Xception_aug.h5'\n",
        "face_classifier = keras.models.load_model(f'models/{model_name}')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/me',y_true='me')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/not_me',y_true='not_me')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct-IMG_0004.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-FB_IMG_1543004113291.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "incorrect-18f3ca8e-f3d6-4020-812b-1800b5b67c15~2.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-FB_IMG_1543004107327.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-a8e5e4f3-4b4b-4d22-ade3-1acd6c74bc10.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-FB_IMG_1543004173806.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-FB_IMG_1543004123855.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-IMG_0077.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-IMG_0007.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "\n",
            "Total accuracy is 55.56%=5/9 samples classified correctly\n",
            "correct-Aicha_El_Ouafi_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Alan_Ball_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aicha_El_Ouafi_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "incorrect-Aitor_Gonzalez_0002.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-Akbar_Hashemi_Rafsanjani_0003.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-Aitor_Gonzalez_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aicha_El_Ouafi_0003.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "\n",
            "Total accuracy is 77.78%=7/9 samples classified correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YXEXc-e5_iJ",
        "outputId": "022594f3-58d3-4504-e036-6a82bb4d2647"
      },
      "source": [
        "face_classifier = ResNet152\n",
        "face_classifier.summary()\n",
        "\n",
        "name_to_save = f\"models/face_classifier_{face_classifier.name}_aug.h5\"\n",
        "\n",
        "face_classifier.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=0.1),metrics=['accuracy'])\n",
        "epochs=500\n",
        "history=face_classifier.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds)\n",
        "face_classifier.save(name_to_save)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ResNet152\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 250, 250, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 256, 256, 3)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 125, 125, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 125, 125, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 125, 125, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 127, 127, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 63, 63, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 63, 63, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 63, 63, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 63, 63, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 63, 63, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 63, 63, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 63, 63, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 63, 63, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 63, 63, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 63, 63, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 63, 63, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 63, 63, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 63, 63, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 63, 63, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 63, 63, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 32, 32, 128)  0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_relu (Activation (None, 32, 32, 128)  0           conv3_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_add (Add)          (None, 32, 32, 512)  0           conv3_block4_out[0][0]           \n",
            "                                                                 conv3_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_out (Activation)   (None, 32, 32, 512)  0           conv3_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 32, 32, 128)  0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_relu (Activation (None, 32, 32, 128)  0           conv3_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_add (Add)          (None, 32, 32, 512)  0           conv3_block5_out[0][0]           \n",
            "                                                                 conv3_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_out (Activation)   (None, 32, 32, 512)  0           conv3_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 32, 32, 128)  0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_relu (Activation (None, 32, 32, 128)  0           conv3_block7_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block7_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block7_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_add (Add)          (None, 32, 32, 512)  0           conv3_block6_out[0][0]           \n",
            "                                                                 conv3_block7_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_out (Activation)   (None, 32, 32, 512)  0           conv3_block7_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block7_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 32, 32, 128)  0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_relu (Activation (None, 32, 32, 128)  0           conv3_block8_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block8_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block8_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_add (Add)          (None, 32, 32, 512)  0           conv3_block7_out[0][0]           \n",
            "                                                                 conv3_block8_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_out (Activation)   (None, 32, 32, 512)  0           conv3_block8_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block8_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block8_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 16, 16, 256)  0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_relu (Activation (None, 16, 16, 256)  0           conv4_block7_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block7_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_add (Add)          (None, 16, 16, 1024) 0           conv4_block6_out[0][0]           \n",
            "                                                                 conv4_block7_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_out (Activation)   (None, 16, 16, 1024) 0           conv4_block7_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block7_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 16, 16, 256)  0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_relu (Activation (None, 16, 16, 256)  0           conv4_block8_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block8_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_add (Add)          (None, 16, 16, 1024) 0           conv4_block7_out[0][0]           \n",
            "                                                                 conv4_block8_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_out (Activation)   (None, 16, 16, 1024) 0           conv4_block8_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block8_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 16, 16, 256)  0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_relu (Activation (None, 16, 16, 256)  0           conv4_block9_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block9_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_add (Add)          (None, 16, 16, 1024) 0           conv4_block8_out[0][0]           \n",
            "                                                                 conv4_block9_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_out (Activation)   (None, 16, 16, 1024) 0           conv4_block9_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block9_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block10_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block10_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_add (Add)         (None, 16, 16, 1024) 0           conv4_block9_out[0][0]           \n",
            "                                                                 conv4_block10_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_out (Activation)  (None, 16, 16, 1024) 0           conv4_block10_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block10_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block11_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block11_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_add (Add)         (None, 16, 16, 1024) 0           conv4_block10_out[0][0]          \n",
            "                                                                 conv4_block11_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_out (Activation)  (None, 16, 16, 1024) 0           conv4_block11_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block11_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block12_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block12_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_add (Add)         (None, 16, 16, 1024) 0           conv4_block11_out[0][0]          \n",
            "                                                                 conv4_block12_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_out (Activation)  (None, 16, 16, 1024) 0           conv4_block12_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block12_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block13_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block13_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_add (Add)         (None, 16, 16, 1024) 0           conv4_block12_out[0][0]          \n",
            "                                                                 conv4_block13_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_out (Activation)  (None, 16, 16, 1024) 0           conv4_block13_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block13_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block14_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block14_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_add (Add)         (None, 16, 16, 1024) 0           conv4_block13_out[0][0]          \n",
            "                                                                 conv4_block14_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_out (Activation)  (None, 16, 16, 1024) 0           conv4_block14_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block14_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block15_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block15_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_add (Add)         (None, 16, 16, 1024) 0           conv4_block14_out[0][0]          \n",
            "                                                                 conv4_block15_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_out (Activation)  (None, 16, 16, 1024) 0           conv4_block15_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block15_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block16_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block16_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_add (Add)         (None, 16, 16, 1024) 0           conv4_block15_out[0][0]          \n",
            "                                                                 conv4_block16_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_out (Activation)  (None, 16, 16, 1024) 0           conv4_block16_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block16_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block17_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block17_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_add (Add)         (None, 16, 16, 1024) 0           conv4_block16_out[0][0]          \n",
            "                                                                 conv4_block17_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_out (Activation)  (None, 16, 16, 1024) 0           conv4_block17_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block17_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block18_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block18_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_add (Add)         (None, 16, 16, 1024) 0           conv4_block17_out[0][0]          \n",
            "                                                                 conv4_block18_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_out (Activation)  (None, 16, 16, 1024) 0           conv4_block18_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block18_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block19_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block19_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_add (Add)         (None, 16, 16, 1024) 0           conv4_block18_out[0][0]          \n",
            "                                                                 conv4_block19_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_out (Activation)  (None, 16, 16, 1024) 0           conv4_block19_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block19_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block20_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block20_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_add (Add)         (None, 16, 16, 1024) 0           conv4_block19_out[0][0]          \n",
            "                                                                 conv4_block20_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_out (Activation)  (None, 16, 16, 1024) 0           conv4_block20_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block20_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block21_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block21_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_add (Add)         (None, 16, 16, 1024) 0           conv4_block20_out[0][0]          \n",
            "                                                                 conv4_block21_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_out (Activation)  (None, 16, 16, 1024) 0           conv4_block21_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block21_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block22_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block22_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_add (Add)         (None, 16, 16, 1024) 0           conv4_block21_out[0][0]          \n",
            "                                                                 conv4_block22_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_out (Activation)  (None, 16, 16, 1024) 0           conv4_block22_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block22_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block23_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block23_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_add (Add)         (None, 16, 16, 1024) 0           conv4_block22_out[0][0]          \n",
            "                                                                 conv4_block23_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_out (Activation)  (None, 16, 16, 1024) 0           conv4_block23_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block23_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block24_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block24_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block24_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_add (Add)         (None, 16, 16, 1024) 0           conv4_block23_out[0][0]          \n",
            "                                                                 conv4_block24_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_out (Activation)  (None, 16, 16, 1024) 0           conv4_block24_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block24_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block25_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block25_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block25_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block25_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block25_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block25_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block25_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_add (Add)         (None, 16, 16, 1024) 0           conv4_block24_out[0][0]          \n",
            "                                                                 conv4_block25_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block25_out (Activation)  (None, 16, 16, 1024) 0           conv4_block25_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block25_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block26_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block26_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block26_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block26_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block26_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block26_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block26_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_add (Add)         (None, 16, 16, 1024) 0           conv4_block25_out[0][0]          \n",
            "                                                                 conv4_block26_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block26_out (Activation)  (None, 16, 16, 1024) 0           conv4_block26_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block26_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block27_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block27_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block27_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block27_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block27_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block27_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block27_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_add (Add)         (None, 16, 16, 1024) 0           conv4_block26_out[0][0]          \n",
            "                                                                 conv4_block27_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block27_out (Activation)  (None, 16, 16, 1024) 0           conv4_block27_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block27_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block28_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block28_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block28_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block28_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block28_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block28_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block28_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_add (Add)         (None, 16, 16, 1024) 0           conv4_block27_out[0][0]          \n",
            "                                                                 conv4_block28_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block28_out (Activation)  (None, 16, 16, 1024) 0           conv4_block28_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block28_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block29_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block29_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block29_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block29_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block29_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block29_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block29_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_add (Add)         (None, 16, 16, 1024) 0           conv4_block28_out[0][0]          \n",
            "                                                                 conv4_block29_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block29_out (Activation)  (None, 16, 16, 1024) 0           conv4_block29_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block29_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block30_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block30_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block30_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block30_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block30_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block30_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block30_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_add (Add)         (None, 16, 16, 1024) 0           conv4_block29_out[0][0]          \n",
            "                                                                 conv4_block30_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block30_out (Activation)  (None, 16, 16, 1024) 0           conv4_block30_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block30_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block31_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block31_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block31_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block31_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block31_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block31_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block31_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_add (Add)         (None, 16, 16, 1024) 0           conv4_block30_out[0][0]          \n",
            "                                                                 conv4_block31_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block31_out (Activation)  (None, 16, 16, 1024) 0           conv4_block31_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block31_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block32_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block32_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block32_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block32_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block32_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block32_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block32_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_add (Add)         (None, 16, 16, 1024) 0           conv4_block31_out[0][0]          \n",
            "                                                                 conv4_block32_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block32_out (Activation)  (None, 16, 16, 1024) 0           conv4_block32_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block32_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block33_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block33_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block33_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block33_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block33_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block33_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block33_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_add (Add)         (None, 16, 16, 1024) 0           conv4_block32_out[0][0]          \n",
            "                                                                 conv4_block33_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block33_out (Activation)  (None, 16, 16, 1024) 0           conv4_block33_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block33_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block34_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block34_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block34_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block34_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block34_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block34_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block34_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_add (Add)         (None, 16, 16, 1024) 0           conv4_block33_out[0][0]          \n",
            "                                                                 conv4_block34_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block34_out (Activation)  (None, 16, 16, 1024) 0           conv4_block34_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block34_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block35_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block35_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block35_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block35_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block35_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block35_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block35_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_add (Add)         (None, 16, 16, 1024) 0           conv4_block34_out[0][0]          \n",
            "                                                                 conv4_block35_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block35_out (Activation)  (None, 16, 16, 1024) 0           conv4_block35_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_1_conv (Conv2D)   (None, 16, 16, 256)  262400      conv4_block35_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_1_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block36_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_1_relu (Activatio (None, 16, 16, 256)  0           conv4_block36_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_2_conv (Conv2D)   (None, 16, 16, 256)  590080      conv4_block36_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_2_bn (BatchNormal (None, 16, 16, 256)  1024        conv4_block36_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_2_relu (Activatio (None, 16, 16, 256)  0           conv4_block36_2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_3_conv (Conv2D)   (None, 16, 16, 1024) 263168      conv4_block36_2_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_3_bn (BatchNormal (None, 16, 16, 1024) 4096        conv4_block36_3_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_add (Add)         (None, 16, 16, 1024) 0           conv4_block35_out[0][0]          \n",
            "                                                                 conv4_block36_3_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block36_out (Activation)  (None, 16, 16, 1024) 0           conv4_block36_add[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block36_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block36_out[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_5 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 2)            4098        global_average_pooling2d_5[0][0] \n",
            "==================================================================================================\n",
            "Total params: 58,375,042\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 58,370,944\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "5/5 [==============================] - 11s 762ms/step - loss: 18.7721 - accuracy: 0.6224 - val_loss: 9.0138 - val_accuracy: 0.7083\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00054\n",
            "Epoch 2/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.8490 - accuracy: 0.9592 - val_loss: 0.4474 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00054\n",
            "Epoch 3/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.6530 - accuracy: 0.9694 - val_loss: 1.9337 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00054\n",
            "Epoch 4/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.4363 - accuracy: 0.9796 - val_loss: 0.5069 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00054\n",
            "Epoch 5/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.9671e-09 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00054 to 0.00000, saving model to models/face_classifier_MobileNet_aug.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to models/face_classifier_MobileNet_aug.h5\n",
            "Epoch 7/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00000\n",
            "Epoch 8/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00000\n",
            "Epoch 9/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00000\n",
            "Epoch 10/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00000\n",
            "Epoch 11/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00000\n",
            "Epoch 12/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00000\n",
            "Epoch 13/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00000\n",
            "Epoch 14/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00000\n",
            "Epoch 15/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00000\n",
            "Epoch 16/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00000\n",
            "Epoch 17/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00000\n",
            "Epoch 18/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00000\n",
            "Epoch 19/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00000\n",
            "Epoch 20/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00000\n",
            "Epoch 21/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00000\n",
            "Epoch 22/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00000\n",
            "Epoch 23/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00000\n",
            "Epoch 24/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00000\n",
            "Epoch 25/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00000\n",
            "Epoch 26/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00000\n",
            "Epoch 27/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00000\n",
            "Epoch 28/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00000\n",
            "Epoch 29/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00000\n",
            "Epoch 30/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00000\n",
            "Epoch 31/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00000\n",
            "Epoch 32/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00000\n",
            "Epoch 33/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00000\n",
            "Epoch 34/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00000\n",
            "Epoch 35/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00000\n",
            "Epoch 36/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00000\n",
            "Epoch 37/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00000\n",
            "Epoch 38/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00000\n",
            "Epoch 39/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00000\n",
            "Epoch 40/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00000\n",
            "Epoch 41/500\n",
            "5/5 [==============================] - 1s 172ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00000\n",
            "Epoch 42/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00000\n",
            "Epoch 43/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00000\n",
            "Epoch 44/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00000\n",
            "Epoch 45/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00000\n",
            "Epoch 46/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00000\n",
            "Epoch 47/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00000\n",
            "Epoch 48/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00000\n",
            "Epoch 49/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00000\n",
            "Epoch 50/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00000\n",
            "Epoch 51/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00000\n",
            "Epoch 52/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00000\n",
            "Epoch 53/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00000\n",
            "Epoch 54/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00000\n",
            "Epoch 55/500\n",
            "5/5 [==============================] - 1s 172ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00000\n",
            "Epoch 56/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00000\n",
            "Epoch 57/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00000\n",
            "Epoch 58/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00000\n",
            "Epoch 59/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00000\n",
            "Epoch 60/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00000\n",
            "Epoch 61/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00000\n",
            "Epoch 62/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00000\n",
            "Epoch 63/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00000\n",
            "Epoch 64/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00000\n",
            "Epoch 65/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00000\n",
            "Epoch 66/500\n",
            "5/5 [==============================] - 1s 173ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00000\n",
            "Epoch 67/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00000\n",
            "Epoch 68/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00000\n",
            "Epoch 69/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00000\n",
            "Epoch 70/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00000\n",
            "Epoch 71/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00000\n",
            "Epoch 72/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00000\n",
            "Epoch 73/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00000\n",
            "Epoch 74/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00000\n",
            "Epoch 75/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00000\n",
            "Epoch 76/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00000\n",
            "Epoch 77/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00000\n",
            "Epoch 78/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00000\n",
            "Epoch 79/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00000\n",
            "Epoch 80/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00000\n",
            "Epoch 81/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00000\n",
            "Epoch 82/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00000\n",
            "Epoch 83/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00000\n",
            "Epoch 84/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00000\n",
            "Epoch 85/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00000\n",
            "Epoch 86/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00000\n",
            "Epoch 87/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00000\n",
            "Epoch 88/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00000\n",
            "Epoch 89/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00000\n",
            "Epoch 90/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00000\n",
            "Epoch 91/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00000\n",
            "Epoch 92/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00000\n",
            "Epoch 93/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00000\n",
            "Epoch 94/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00000\n",
            "Epoch 95/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00000\n",
            "Epoch 96/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00000\n",
            "Epoch 97/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00000\n",
            "Epoch 98/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00000\n",
            "Epoch 99/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00000\n",
            "Epoch 100/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00000\n",
            "Epoch 101/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00000\n",
            "Epoch 102/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00000\n",
            "Epoch 103/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.00000\n",
            "Epoch 104/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.00000\n",
            "Epoch 105/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.00000\n",
            "Epoch 106/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.00000\n",
            "Epoch 107/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.00000\n",
            "Epoch 108/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.00000\n",
            "Epoch 109/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.00000\n",
            "Epoch 110/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.00000\n",
            "Epoch 111/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.00000\n",
            "Epoch 112/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.00000\n",
            "Epoch 113/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.00000\n",
            "Epoch 114/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.00000\n",
            "Epoch 115/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.00000\n",
            "Epoch 116/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.00000\n",
            "Epoch 117/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.00000\n",
            "Epoch 118/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.00000\n",
            "Epoch 119/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.00000\n",
            "Epoch 120/500\n",
            "5/5 [==============================] - 1s 172ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.00000\n",
            "Epoch 121/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.00000\n",
            "Epoch 122/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.00000\n",
            "Epoch 123/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.00000\n",
            "Epoch 124/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.00000\n",
            "Epoch 125/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.00000\n",
            "Epoch 126/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.00000\n",
            "Epoch 127/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.00000\n",
            "Epoch 128/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.00000\n",
            "Epoch 129/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.00000\n",
            "Epoch 130/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.00000\n",
            "Epoch 131/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.00000\n",
            "Epoch 132/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.00000\n",
            "Epoch 133/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.00000\n",
            "Epoch 134/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.00000\n",
            "Epoch 135/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.00000\n",
            "Epoch 136/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.00000\n",
            "Epoch 137/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.00000\n",
            "Epoch 138/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.00000\n",
            "Epoch 139/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.00000\n",
            "Epoch 140/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.00000\n",
            "Epoch 141/500\n",
            "5/5 [==============================] - 1s 189ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.00000\n",
            "Epoch 142/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.00000\n",
            "Epoch 143/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.00000\n",
            "Epoch 144/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.00000\n",
            "Epoch 145/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.00000\n",
            "Epoch 146/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.00000\n",
            "Epoch 147/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.00000\n",
            "Epoch 148/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.00000\n",
            "Epoch 149/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00000\n",
            "Epoch 150/500\n",
            "5/5 [==============================] - 1s 172ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00000\n",
            "Epoch 151/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00000\n",
            "Epoch 152/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00000\n",
            "Epoch 153/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00000\n",
            "Epoch 154/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00000\n",
            "Epoch 155/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00000\n",
            "Epoch 156/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00000\n",
            "Epoch 157/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00000\n",
            "Epoch 158/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00000\n",
            "Epoch 159/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00000\n",
            "Epoch 160/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00000\n",
            "Epoch 161/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00000\n",
            "Epoch 162/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00000\n",
            "Epoch 163/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00000\n",
            "Epoch 164/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00000\n",
            "Epoch 165/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.00000\n",
            "Epoch 166/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00000\n",
            "Epoch 167/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00000\n",
            "Epoch 168/500\n",
            "5/5 [==============================] - 1s 166ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00000\n",
            "Epoch 169/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00000\n",
            "Epoch 170/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00000\n",
            "Epoch 171/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00000\n",
            "Epoch 172/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00000\n",
            "Epoch 173/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00000\n",
            "Epoch 174/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00000\n",
            "Epoch 175/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00000\n",
            "Epoch 176/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00000\n",
            "Epoch 177/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00000\n",
            "Epoch 178/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00000\n",
            "Epoch 179/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00000\n",
            "Epoch 180/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00000\n",
            "Epoch 181/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00000\n",
            "Epoch 182/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00000\n",
            "Epoch 183/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00000\n",
            "Epoch 184/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00000\n",
            "Epoch 185/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00000\n",
            "Epoch 186/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00000\n",
            "Epoch 187/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00000\n",
            "Epoch 188/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00000\n",
            "Epoch 189/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00000\n",
            "Epoch 190/500\n",
            "5/5 [==============================] - 1s 172ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00000\n",
            "Epoch 191/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00000\n",
            "Epoch 192/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00000\n",
            "Epoch 193/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00000\n",
            "Epoch 194/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00000\n",
            "Epoch 195/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00000\n",
            "Epoch 196/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00000\n",
            "Epoch 197/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00000\n",
            "Epoch 198/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00000\n",
            "Epoch 199/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00000\n",
            "Epoch 200/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00000\n",
            "Epoch 201/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.00000\n",
            "Epoch 202/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.00000\n",
            "Epoch 203/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.00000\n",
            "Epoch 204/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.00000\n",
            "Epoch 205/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.00000\n",
            "Epoch 206/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.00000\n",
            "Epoch 207/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.00000\n",
            "Epoch 208/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.00000\n",
            "Epoch 209/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.00000\n",
            "Epoch 210/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.00000\n",
            "Epoch 211/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.00000\n",
            "Epoch 212/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.00000\n",
            "Epoch 213/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.00000\n",
            "Epoch 214/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.00000\n",
            "Epoch 215/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.00000\n",
            "Epoch 216/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.00000\n",
            "Epoch 217/500\n",
            "5/5 [==============================] - 1s 172ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.00000\n",
            "Epoch 218/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.00000\n",
            "Epoch 219/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.00000\n",
            "Epoch 220/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.00000\n",
            "Epoch 221/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.00000\n",
            "Epoch 222/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.00000\n",
            "Epoch 223/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.00000\n",
            "Epoch 224/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.00000\n",
            "Epoch 225/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.00000\n",
            "Epoch 226/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.00000\n",
            "Epoch 227/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.00000\n",
            "Epoch 228/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.00000\n",
            "Epoch 229/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.00000\n",
            "Epoch 230/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.00000\n",
            "Epoch 231/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.00000\n",
            "Epoch 232/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.00000\n",
            "Epoch 233/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.00000\n",
            "Epoch 234/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.00000\n",
            "Epoch 235/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.00000\n",
            "Epoch 236/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.00000\n",
            "Epoch 237/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.00000\n",
            "Epoch 238/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.00000\n",
            "Epoch 239/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.00000\n",
            "Epoch 240/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.00000\n",
            "Epoch 241/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.00000\n",
            "Epoch 242/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.00000\n",
            "Epoch 243/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.00000\n",
            "Epoch 244/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.00000\n",
            "Epoch 245/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.00000\n",
            "Epoch 246/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.00000\n",
            "Epoch 247/500\n",
            "5/5 [==============================] - 1s 168ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.00000\n",
            "Epoch 248/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.00000\n",
            "Epoch 249/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.00000\n",
            "Epoch 250/500\n",
            "5/5 [==============================] - 1s 167ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.00000\n",
            "Epoch 251/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.00000\n",
            "Epoch 252/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.00000\n",
            "Epoch 253/500\n",
            "5/5 [==============================] - 1s 171ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.00000\n",
            "Epoch 254/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.00000\n",
            "Epoch 255/500\n",
            "5/5 [==============================] - 1s 170ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.00000\n",
            "Epoch 256/500\n",
            "5/5 [==============================] - 1s 169ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.00000\n",
            "Epoch 00256: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgF1RsvF6KRk",
        "outputId": "a2ab38ee-c26a-4352-d407-b454c047b0cb"
      },
      "source": [
        "model_name = 'face_classifier_ResNet152_aug.h5'\n",
        "face_classifier = keras.models.load_model(f'models/{model_name}')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/me',y_true='me')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/not_me',y_true='not_me')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct-IMG_0004.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004113291.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-18f3ca8e-f3d6-4020-812b-1800b5b67c15~2.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004107327.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-a8e5e4f3-4b4b-4d22-ade3-1acd6c74bc10.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004173806.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004123855.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-IMG_0077.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-IMG_0007.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "\n",
            "Total accuracy is 100.00%=9/9 samples classified correctly\n",
            "correct-Aicha_El_Ouafi_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Alan_Ball_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0001.jpg\n",
            "\tme     with probability of 4.41%\n",
            "\tnot_me with probability of 96.89%\n",
            "correct-Aicha_El_Ouafi_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aitor_Gonzalez_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0003.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aitor_Gonzalez_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "incorrect-Aicha_El_Ouafi_0003.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "\n",
            "Total accuracy is 88.89%=8/9 samples classified correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3vh_MxY7sIG",
        "outputId": "cdfdc868-6f84-4b1b-fac0-545116b94f66"
      },
      "source": [
        "face_classifier = ResNet50\n",
        "face_classifier.summary()\n",
        "\n",
        "name_to_save = f\"models/face_classifier_{face_classifier.name}_aug.h5\"\n",
        "\n",
        "face_classifier.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=0.1),metrics=['accuracy'])\n",
        "epochs=500\n",
        "history=face_classifier.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds)\n",
        "face_classifier.save(name_to_save)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ResNet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 250, 250, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 256, 256, 3)  0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 125, 125, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 125, 125, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 125, 125, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 127, 127, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 63, 63, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 63, 63, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 63, 63, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 63, 63, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 63, 63, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 63, 63, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 63, 63, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 63, 63, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 63, 63, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 63, 63, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 63, 63, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 63, 63, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 63, 63, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 63, 63, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 63, 63, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 63, 63, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 63, 63, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 63, 63, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 63, 63, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 63, 63, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 2)            4098        global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/500\n",
            "5/5 [==============================] - 4s 281ms/step - loss: 2.2592 - accuracy: 0.7653 - val_loss: 1.8371 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.83708, saving model to models/face_classifier_ResNet152_aug.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 1.7989 - accuracy: 0.9388 - val_loss: 4.9221e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.83708 to 0.00000, saving model to models/face_classifier_ResNet152_aug.h5\n",
            "Epoch 3/500\n",
            "5/5 [==============================] - 1s 92ms/step - loss: 5.9622e-04 - accuracy: 1.0000 - val_loss: 0.3374 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00000\n",
            "Epoch 4/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.7403 - accuracy: 0.9796 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to models/face_classifier_ResNet152_aug.h5\n",
            "Epoch 5/500\n",
            "5/5 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00000\n",
            "Epoch 6/500\n",
            "5/5 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00000\n",
            "Epoch 7/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00000\n",
            "Epoch 8/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00000\n",
            "Epoch 9/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00000\n",
            "Epoch 10/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00000\n",
            "Epoch 11/500\n",
            "5/5 [==============================] - 1s 86ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00000\n",
            "Epoch 12/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00000\n",
            "Epoch 13/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00000\n",
            "Epoch 14/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00000\n",
            "Epoch 15/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00000\n",
            "Epoch 16/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00000\n",
            "Epoch 17/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00000\n",
            "Epoch 18/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00000\n",
            "Epoch 19/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00000\n",
            "Epoch 20/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00000\n",
            "Epoch 21/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00000\n",
            "Epoch 22/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00000\n",
            "Epoch 23/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00000\n",
            "Epoch 24/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00000\n",
            "Epoch 25/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00000\n",
            "Epoch 26/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00000\n",
            "Epoch 27/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00000\n",
            "Epoch 28/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00000\n",
            "Epoch 29/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00000\n",
            "Epoch 30/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00000\n",
            "Epoch 31/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00000\n",
            "Epoch 32/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00000\n",
            "Epoch 33/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00000\n",
            "Epoch 34/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00000\n",
            "Epoch 35/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00000\n",
            "Epoch 36/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00000\n",
            "Epoch 37/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00000\n",
            "Epoch 38/500\n",
            "5/5 [==============================] - 1s 86ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00000\n",
            "Epoch 39/500\n",
            "5/5 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00000\n",
            "Epoch 40/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00000\n",
            "Epoch 41/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00000\n",
            "Epoch 42/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00000\n",
            "Epoch 43/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00000\n",
            "Epoch 44/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00000\n",
            "Epoch 45/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00000\n",
            "Epoch 46/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00000\n",
            "Epoch 47/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00000\n",
            "Epoch 48/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00000\n",
            "Epoch 49/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00000\n",
            "Epoch 50/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00000\n",
            "Epoch 51/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00000\n",
            "Epoch 52/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00000\n",
            "Epoch 53/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00000\n",
            "Epoch 54/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00000\n",
            "Epoch 55/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00000\n",
            "Epoch 56/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00000\n",
            "Epoch 57/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00000\n",
            "Epoch 58/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00000\n",
            "Epoch 59/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00000\n",
            "Epoch 60/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00000\n",
            "Epoch 61/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00000\n",
            "Epoch 62/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00000\n",
            "Epoch 63/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00000\n",
            "Epoch 64/500\n",
            "5/5 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00000\n",
            "Epoch 65/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00000\n",
            "Epoch 66/500\n",
            "5/5 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00000\n",
            "Epoch 67/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00000\n",
            "Epoch 68/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00000\n",
            "Epoch 69/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00000\n",
            "Epoch 70/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00000\n",
            "Epoch 71/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00000\n",
            "Epoch 72/500\n",
            "5/5 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00000\n",
            "Epoch 73/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00000\n",
            "Epoch 74/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00000\n",
            "Epoch 75/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00000\n",
            "Epoch 76/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00000\n",
            "Epoch 77/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00000\n",
            "Epoch 78/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00000\n",
            "Epoch 79/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00000\n",
            "Epoch 80/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00000\n",
            "Epoch 81/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00000\n",
            "Epoch 82/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00000\n",
            "Epoch 83/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00000\n",
            "Epoch 84/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00000\n",
            "Epoch 85/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00000\n",
            "Epoch 86/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00000\n",
            "Epoch 87/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00000\n",
            "Epoch 88/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00000\n",
            "Epoch 89/500\n",
            "5/5 [==============================] - 1s 90ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00000\n",
            "Epoch 90/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00000\n",
            "Epoch 91/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00000\n",
            "Epoch 92/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00000\n",
            "Epoch 93/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00000\n",
            "Epoch 94/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00000\n",
            "Epoch 95/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00000\n",
            "Epoch 96/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00000\n",
            "Epoch 97/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00000\n",
            "Epoch 98/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00000\n",
            "Epoch 99/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00000\n",
            "Epoch 100/500\n",
            "5/5 [==============================] - 1s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00000\n",
            "Epoch 101/500\n",
            "5/5 [==============================] - 1s 89ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00000\n",
            "Epoch 102/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00000\n",
            "Epoch 103/500\n",
            "5/5 [==============================] - 1s 87ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.00000\n",
            "Epoch 104/500\n",
            "5/5 [==============================] - 1s 88ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.00000\n",
            "Epoch 00104: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTLkYfpP733T",
        "outputId": "af3f7bd3-6e6e-4f7e-cba4-d22f8ceda35d"
      },
      "source": [
        "model_name = 'face_classifier_ResNet50_aug.h5'\n",
        "face_classifier = keras.models.load_model(f'models/{model_name}')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/me',y_true='me')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/not_me',y_true='not_me')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct-IMG_0004.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004113291.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-18f3ca8e-f3d6-4020-812b-1800b5b67c15~2.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-FB_IMG_1543004107327.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-a8e5e4f3-4b4b-4d22-ade3-1acd6c74bc10.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004173806.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004123855.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-IMG_0077.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "incorrect-IMG_0007.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "\n",
            "Total accuracy is 77.78%=7/9 samples classified correctly\n",
            "correct-Aicha_El_Ouafi_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Alan_Ball_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aicha_El_Ouafi_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aitor_Gonzalez_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0003.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aitor_Gonzalez_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aicha_El_Ouafi_0003.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "\n",
            "Total accuracy is 100.00%=9/9 samples classified correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uby9w5g68jTY",
        "outputId": "195a8e0e-a85d-4b57-a75d-2500c415dbc1"
      },
      "source": [
        "face_classifier = VGG16\n",
        "face_classifier.summary()\n",
        "\n",
        "name_to_save = f\"models/face_classifier_{face_classifier.name}_aug.h5\"\n",
        "\n",
        "face_classifier.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=0.1),metrics=['accuracy'])\n",
        "epochs=500\n",
        "history=face_classifier.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds)\n",
        "face_classifier.save(name_to_save)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"VGG16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 250, 250, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 250, 250, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 250, 250, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 125, 125, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 125, 125, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 125, 125, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 62, 62, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 62, 62, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 62, 62, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 62, 62, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 31, 31, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 31, 31, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 15, 15, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 134,268,738\n",
            "Trainable params: 119,554,050\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "5/5 [==============================] - 7s 865ms/step - loss: 10298608.0000 - accuracy: 0.3878 - val_loss: 208188.4375 - val_accuracy: 0.5833\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00000\n",
            "Epoch 2/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 253935.2188 - accuracy: 0.8061 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00000\n",
            "Epoch 3/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 98477.0703 - accuracy: 0.9286 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00000\n",
            "Epoch 4/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 52366.6445 - accuracy: 0.9694 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00000\n",
            "Epoch 5/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00000\n",
            "Epoch 6/500\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00000\n",
            "Epoch 7/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00000\n",
            "Epoch 8/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00000\n",
            "Epoch 9/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 7613.1455 - accuracy: 0.9898 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00000\n",
            "Epoch 10/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00000\n",
            "Epoch 11/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 156979.8750 - accuracy: 0.9898 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00000\n",
            "Epoch 12/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 6900.2617 - accuracy: 0.9898 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00000\n",
            "Epoch 13/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 82082.3125 - accuracy: 0.9694 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00000\n",
            "Epoch 14/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00000\n",
            "Epoch 15/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00000\n",
            "Epoch 16/500\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00000\n",
            "Epoch 17/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 45957.8711 - accuracy: 0.9898 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00000\n",
            "Epoch 18/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00000\n",
            "Epoch 19/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00000\n",
            "Epoch 20/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 176903.5469 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00000\n",
            "Epoch 21/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 383523.2812 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00000\n",
            "Epoch 22/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 521421.2500 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00000\n",
            "Epoch 23/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 606070.3125 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00000\n",
            "Epoch 24/500\n",
            "5/5 [==============================] - 1s 121ms/step - loss: 8198.5342 - accuracy: 0.9796 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00000\n",
            "Epoch 25/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00000\n",
            "Epoch 26/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00000\n",
            "Epoch 27/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 58861.4688 - accuracy: 0.9898 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00000\n",
            "Epoch 28/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00000\n",
            "Epoch 29/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00000\n",
            "Epoch 30/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00000\n",
            "Epoch 31/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00000\n",
            "Epoch 32/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00000\n",
            "Epoch 33/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00000\n",
            "Epoch 34/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00000\n",
            "Epoch 35/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00000\n",
            "Epoch 36/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00000\n",
            "Epoch 37/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00000\n",
            "Epoch 38/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00000\n",
            "Epoch 39/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00000\n",
            "Epoch 40/500\n",
            "5/5 [==============================] - 1s 115ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00000\n",
            "Epoch 41/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00000\n",
            "Epoch 42/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00000\n",
            "Epoch 43/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00000\n",
            "Epoch 44/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00000\n",
            "Epoch 45/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00000\n",
            "Epoch 46/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00000\n",
            "Epoch 47/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00000\n",
            "Epoch 48/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00000\n",
            "Epoch 49/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00000\n",
            "Epoch 50/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00000\n",
            "Epoch 51/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00000\n",
            "Epoch 52/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00000\n",
            "Epoch 53/500\n",
            "5/5 [==============================] - 1s 123ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00000\n",
            "Epoch 54/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00000\n",
            "Epoch 55/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00000\n",
            "Epoch 56/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00000\n",
            "Epoch 57/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00000\n",
            "Epoch 58/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00000\n",
            "Epoch 59/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00000\n",
            "Epoch 60/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00000\n",
            "Epoch 61/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00000\n",
            "Epoch 62/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00000\n",
            "Epoch 63/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00000\n",
            "Epoch 64/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00000\n",
            "Epoch 65/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00000\n",
            "Epoch 66/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00000\n",
            "Epoch 67/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00000\n",
            "Epoch 68/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00000\n",
            "Epoch 69/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00000\n",
            "Epoch 70/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00000\n",
            "Epoch 71/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00000\n",
            "Epoch 72/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00000\n",
            "Epoch 73/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00000\n",
            "Epoch 74/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00000\n",
            "Epoch 75/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00000\n",
            "Epoch 76/500\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00000\n",
            "Epoch 77/500\n",
            "5/5 [==============================] - 1s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00000\n",
            "Epoch 78/500\n",
            "5/5 [==============================] - 1s 122ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00000\n",
            "Epoch 79/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00000\n",
            "Epoch 80/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00000\n",
            "Epoch 81/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00000\n",
            "Epoch 82/500\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00000\n",
            "Epoch 83/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00000\n",
            "Epoch 84/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00000\n",
            "Epoch 85/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00000\n",
            "Epoch 86/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00000\n",
            "Epoch 87/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00000\n",
            "Epoch 88/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00000\n",
            "Epoch 89/500\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00000\n",
            "Epoch 90/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00000\n",
            "Epoch 91/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00000\n",
            "Epoch 92/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00000\n",
            "Epoch 93/500\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00000\n",
            "Epoch 94/500\n",
            "5/5 [==============================] - 1s 117ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00000\n",
            "Epoch 95/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00000\n",
            "Epoch 96/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00000\n",
            "Epoch 97/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00000\n",
            "Epoch 98/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00000\n",
            "Epoch 99/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00000\n",
            "Epoch 100/500\n",
            "5/5 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00000\n",
            "Epoch 101/500\n",
            "5/5 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.00000\n",
            "Epoch 102/500\n",
            "5/5 [==============================] - 1s 120ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.00000\n",
            "Epoch 00102: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7DmYrqM8yCz",
        "outputId": "b9229469-7b3f-4b8d-c970-6e4fbb647fcc"
      },
      "source": [
        "model_name = 'face_classifier_VGG16_aug.h5'\n",
        "face_classifier = keras.models.load_model(f'models/{model_name}')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/me',y_true='me')\n",
        "test_image_classifier_with_folder(face_classifier,'dataset/testImages/not_me',y_true='not_me')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct-IMG_0004.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004113291.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-18f3ca8e-f3d6-4020-812b-1800b5b67c15~2.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004107327.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-a8e5e4f3-4b4b-4d22-ade3-1acd6c74bc10.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004173806.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-FB_IMG_1543004123855.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-IMG_0077.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "correct-IMG_0007.jpg\n",
            "\tme     with probability of 100.00%\n",
            "\tnot_me with probability of 0.00%\n",
            "\n",
            "Total accuracy is 100.00%=9/9 samples classified correctly\n",
            "correct-Aicha_El_Ouafi_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Alan_Ball_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aicha_El_Ouafi_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aitor_Gonzalez_0002.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Akbar_Hashemi_Rafsanjani_0003.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aitor_Gonzalez_0001.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "correct-Aicha_El_Ouafi_0003.jpg\n",
            "\tme     with probability of 0.00%\n",
            "\tnot_me with probability of 100.00%\n",
            "\n",
            "Total accuracy is 100.00%=9/9 samples classified correctly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6rQUBt99Qwy",
        "outputId": "d4f3b9c2-598a-41d3-ca76-34dd686a7f70"
      },
      "source": [
        "!zip -r /content/models.zip /content/models"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/face_classifier_ResNet50_aug.h5 (deflated 8%)\n",
            "  adding: content/models/face_classifier_MobileNet_aug.h5 (deflated 8%)\n",
            "  adding: content/models/face_classifier_ResNet152_aug.h5 (deflated 8%)\n",
            "  adding: content/models/face_classifier_Xception_aug.h5 (deflated 8%)\n",
            "  adding: content/models/face_classifier_VGG16_aug.h5 (deflated 21%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R5sHl1x9Z7P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}